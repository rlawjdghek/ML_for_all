{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5장 Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(42)\n",
    "def download_data(data_URL,data_PATH,file_name):\n",
    "    os.makedirs(data_PATH, exist_ok=True)  #디렉터리를 만드는함수. exist_ok =True로한다면 이미\n",
    "    #있는 파일이면 에러메세지를 뜨게 하지 않는다.\n",
    "    file_path = os.path.join(data_PATH,file_name)\n",
    "    if not os.path.exists(file_path):  #만약 경로가 존재하지 않는다면, 즉 words.zip파일이 없다면\n",
    "        urllib.request.urlretrieve(data_URL,file_path)  #zip_path경로로 url을 다운받는다.\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "Y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None,2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name= 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "cost  = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "\n",
    "train= tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5 , dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1.0183212\n",
      "200 : 0.29011247\n",
      "400 : 0.22623175\n",
      "600 : 0.18459295\n",
      "800 : 0.15569328\n",
      "1000 : 0.13461415\n",
      "hypothsis : [[0.02470729]\n",
      " [0.14976779]\n",
      " [0.27454156]\n",
      " [0.79559934]\n",
      " [0.9481783 ]\n",
      " [0.98309255]], Y_pred : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], acc_val : 1.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1001):\n",
    "        cost_val , _ = sess.run([cost, train], feed_dict=  {X : X_data, Y: Y_data})\n",
    "        if(step % 200 ==0):\n",
    "            print(step , ':' , cost_val)\n",
    "    hypo_val, Y_pred, acc_val = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:X_data,\n",
    "                                                                                        Y:Y_data})\n",
    "    print(\"hypothsis : {}, Y_pred : {}, acc_val : {}\".format(hypo_val, Y_pred, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "def fetch_words_data(WORDS_URL,WORDS_PATH):\n",
    "    os.makedirs(WORDS_PATH, exist_ok=True)  #디렉터리를 만드는함수. exist_ok =True로한다면 이미\n",
    "    #있는 파일이면 에러메세지를 뜨게 하지 않는다.\n",
    "    file_name = '2주차_data_03.csv'\n",
    "    file_path = os.path.join(WORDS_PATH,file_name)\n",
    "    if not os.path.exists(file_path):  #만약 경로가 존재하지 않는다면, 즉 words.zip파일이 없다면\n",
    "        urllib.request.urlretrieve(WORDS_URL,file_path)  #zip_path경로로 url을 다운받는다.\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/hunkim/DeepLearningZeroToAll/master/data-03-diabetes.csv'\n",
    "path = './datasets/'\n",
    "name = 'diabete.csv'\n",
    "download_data(url, path,name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('./datasets/diabete.csv',delimiter=',',dtype  = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = xy[:,:-1]\n",
    "Y_data = xy[:,-1]\n",
    "X = tf.placeholder(tf.float32, shape = [None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) +b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y* tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=  tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1.0175186\n",
      "200 : 0.8062727\n",
      "400 : 0.7634332\n",
      "600 : 0.7491797\n",
      "800 : 0.7403433\n",
      "1000 : 0.7329285\n",
      "1200 : 0.7261893\n",
      "1400 : 0.7199638\n",
      "1600 : 0.7141975\n",
      "1800 : 0.70885736\n",
      "2000 : 0.70391405\n",
      "Y_proba : [[0.6035249 ]\n",
      " [0.8740468 ]\n",
      " [0.47547624]\n",
      " [0.8085954 ]\n",
      " [0.23391217]\n",
      " [0.7599505 ]\n",
      " [0.8026402 ]\n",
      " [0.7418236 ]\n",
      " [0.5585134 ]\n",
      " [0.6451629 ]\n",
      " [0.6844182 ]\n",
      " [0.43716276]\n",
      " [0.4442201 ]\n",
      " [0.7838383 ]\n",
      " [0.7656963 ]\n",
      " [0.46455222]\n",
      " [0.7567379 ]\n",
      " [0.70204663]\n",
      " [0.60072815]\n",
      " [0.4722822 ]\n",
      " [0.71889186]\n",
      " [0.31326532]\n",
      " [0.7549449 ]\n",
      " [0.6577891 ]\n",
      " [0.5661048 ]\n",
      " [0.79788613]\n",
      " [0.63363224]\n",
      " [0.6581116 ]\n",
      " [0.76261073]\n",
      " [0.42698073]\n",
      " [0.830772  ]\n",
      " [0.8169991 ]\n",
      " [0.73804283]\n",
      " [0.68149006]\n",
      " [0.59840775]\n",
      " [0.734498  ]\n",
      " [0.767215  ]\n",
      " [0.42575797]\n",
      " [0.4212088 ]\n",
      " [0.5095249 ]\n",
      " [0.84922844]\n",
      " [0.29269552]\n",
      " [0.61648715]\n",
      " [0.20939201]\n",
      " [0.62393653]\n",
      " [0.8708092 ]\n",
      " [0.7772002 ]\n",
      " [0.75688195]\n",
      " [0.7666739 ]\n",
      " [0.7692908 ]\n",
      " [0.77541625]\n",
      " [0.42592055]\n",
      " [0.51107574]\n",
      " [0.9428344 ]\n",
      " [0.42573476]\n",
      " [0.33077967]\n",
      " [0.27428144]\n",
      " [0.6346148 ]\n",
      " [0.824351  ]\n",
      " [0.6602686 ]\n",
      " [0.89184093]\n",
      " [0.5523355 ]\n",
      " [0.73866963]\n",
      " [0.84590733]\n",
      " [0.68239   ]\n",
      " [0.55461526]\n",
      " [0.8090718 ]\n",
      " [0.5552494 ]\n",
      " [0.64670193]\n",
      " [0.5769917 ]\n",
      " [0.514883  ]\n",
      " [0.6765956 ]\n",
      " [0.84996915]\n",
      " [0.9028758 ]\n",
      " [0.8356547 ]\n",
      " [0.7893143 ]\n",
      " [0.6235744 ]\n",
      " [0.84460187]\n",
      " [0.9022906 ]\n",
      " [0.8578738 ]\n",
      " [0.68198574]\n",
      " [0.8565833 ]\n",
      " [0.47320834]\n",
      " [0.6071113 ]\n",
      " [0.7223953 ]\n",
      " [0.6741183 ]\n",
      " [0.5565425 ]\n",
      " [0.8672081 ]\n",
      " [0.88923097]\n",
      " [0.6724693 ]\n",
      " [0.61245567]\n",
      " [0.72077477]\n",
      " [0.53056973]\n",
      " [0.61568826]\n",
      " [0.8701632 ]\n",
      " [0.87685156]\n",
      " [0.7516354 ]\n",
      " [0.4436154 ]\n",
      " [0.34432688]\n",
      " [0.6821992 ]\n",
      " [0.7012111 ]\n",
      " [0.7985927 ]\n",
      " [0.69258016]\n",
      " [0.61971647]\n",
      " [0.7500546 ]\n",
      " [0.62406254]\n",
      " [0.72461194]\n",
      " [0.6473587 ]\n",
      " [0.4533611 ]\n",
      " [0.676597  ]\n",
      " [0.7090876 ]\n",
      " [0.8202167 ]\n",
      " [0.5080902 ]\n",
      " [0.5452263 ]\n",
      " [0.69250697]\n",
      " [0.80572665]\n",
      " [0.8532845 ]\n",
      " [0.7858602 ]\n",
      " [0.17929956]\n",
      " [0.7709234 ]\n",
      " [0.6452615 ]\n",
      " [0.7117631 ]\n",
      " [0.70746285]\n",
      " [0.6220508 ]\n",
      " [0.55890566]\n",
      " [0.6442887 ]\n",
      " [0.62689674]\n",
      " [0.6884156 ]\n",
      " [0.54148674]\n",
      " [0.5784938 ]\n",
      " [0.471031  ]\n",
      " [0.80613124]\n",
      " [0.7594762 ]\n",
      " [0.72213906]\n",
      " [0.6556973 ]\n",
      " [0.73083854]\n",
      " [0.58445275]\n",
      " [0.77134264]\n",
      " [0.7421558 ]\n",
      " [0.7583471 ]\n",
      " [0.70627356]\n",
      " [0.752249  ]\n",
      " [0.6188812 ]\n",
      " [0.7904005 ]\n",
      " [0.8906895 ]\n",
      " [0.5023507 ]\n",
      " [0.61793876]\n",
      " [0.8985902 ]\n",
      " [0.48641375]\n",
      " [0.7703917 ]\n",
      " [0.3203411 ]\n",
      " [0.5029818 ]\n",
      " [0.37262982]\n",
      " [0.4539393 ]\n",
      " [0.78267765]\n",
      " [0.7028718 ]\n",
      " [0.7735561 ]\n",
      " [0.27697504]\n",
      " [0.62066066]\n",
      " [0.642403  ]\n",
      " [0.6487117 ]\n",
      " [0.85367835]\n",
      " [0.54941595]\n",
      " [0.6943902 ]\n",
      " [0.71541023]\n",
      " [0.66207   ]\n",
      " [0.7225952 ]\n",
      " [0.6700141 ]\n",
      " [0.7643248 ]\n",
      " [0.5576362 ]\n",
      " [0.83957803]\n",
      " [0.5983154 ]\n",
      " [0.79271543]\n",
      " [0.29700992]\n",
      " [0.7898782 ]\n",
      " [0.24294701]\n",
      " [0.5585154 ]\n",
      " [0.48699138]\n",
      " [0.844172  ]\n",
      " [0.5916104 ]\n",
      " [0.7343334 ]\n",
      " [0.8654182 ]\n",
      " [0.67017764]\n",
      " [0.4239097 ]\n",
      " [0.54668653]\n",
      " [0.2983438 ]\n",
      " [0.59146154]\n",
      " [0.53884214]\n",
      " [0.8208246 ]\n",
      " [0.5413263 ]\n",
      " [0.57492673]\n",
      " [0.49553514]\n",
      " [0.9158358 ]\n",
      " [0.4028332 ]\n",
      " [0.8303389 ]\n",
      " [0.7217362 ]\n",
      " [0.50201464]\n",
      " [0.7026435 ]\n",
      " [0.743262  ]\n",
      " [0.5882418 ]\n",
      " [0.788253  ]\n",
      " [0.7954065 ]\n",
      " [0.69401985]\n",
      " [0.81583536]\n",
      " [0.34632772]\n",
      " [0.48044068]\n",
      " [0.74000067]\n",
      " [0.47931957]\n",
      " [0.89775175]\n",
      " [0.469861  ]\n",
      " [0.5343431 ]\n",
      " [0.5019569 ]\n",
      " [0.6538397 ]\n",
      " [0.40936944]\n",
      " [0.5990539 ]\n",
      " [0.61249346]\n",
      " [0.7728901 ]\n",
      " [0.7209829 ]\n",
      " [0.4805069 ]\n",
      " [0.45671588]\n",
      " [0.7747603 ]\n",
      " [0.56886506]\n",
      " [0.7341712 ]\n",
      " [0.7286511 ]\n",
      " [0.744998  ]\n",
      " [0.5509965 ]\n",
      " [0.24556172]\n",
      " [0.5422004 ]\n",
      " [0.45311052]\n",
      " [0.5932791 ]\n",
      " [0.7325624 ]\n",
      " [0.6751131 ]\n",
      " [0.7796452 ]\n",
      " [0.41205683]\n",
      " [0.38129154]\n",
      " [0.53839767]\n",
      " [0.765918  ]\n",
      " [0.8848743 ]\n",
      " [0.6879521 ]\n",
      " [0.69761   ]\n",
      " [0.5983907 ]\n",
      " [0.50927556]\n",
      " [0.4253798 ]\n",
      " [0.7141858 ]\n",
      " [0.53548944]\n",
      " [0.723811  ]\n",
      " [0.8267876 ]\n",
      " [0.7741556 ]\n",
      " [0.6781518 ]\n",
      " [0.7917705 ]\n",
      " [0.83705777]\n",
      " [0.79938245]\n",
      " [0.75694644]\n",
      " [0.7690896 ]\n",
      " [0.8471217 ]\n",
      " [0.57931334]\n",
      " [0.45975563]\n",
      " [0.58901906]\n",
      " [0.7890433 ]\n",
      " [0.76324373]\n",
      " [0.72931355]\n",
      " [0.61460406]\n",
      " [0.51746863]\n",
      " [0.5345591 ]\n",
      " [0.6924522 ]\n",
      " [0.679872  ]\n",
      " [0.55623364]\n",
      " [0.7421739 ]\n",
      " [0.74684316]\n",
      " [0.67388046]\n",
      " [0.74477446]\n",
      " [0.47667685]\n",
      " [0.8525115 ]\n",
      " [0.72277474]\n",
      " [0.69853854]\n",
      " [0.7768033 ]\n",
      " [0.5696007 ]\n",
      " [0.5816331 ]\n",
      " [0.6360003 ]\n",
      " [0.5661113 ]\n",
      " [0.7685915 ]\n",
      " [0.53658056]\n",
      " [0.4484856 ]\n",
      " [0.8063265 ]\n",
      " [0.59276724]\n",
      " [0.6692383 ]\n",
      " [0.56786174]\n",
      " [0.36060503]\n",
      " [0.5344915 ]\n",
      " [0.6878265 ]\n",
      " [0.44382915]\n",
      " [0.655697  ]\n",
      " [0.6002558 ]\n",
      " [0.6902398 ]\n",
      " [0.69688547]\n",
      " [0.45230556]\n",
      " [0.5613076 ]\n",
      " [0.69322705]\n",
      " [0.5541125 ]\n",
      " [0.6796129 ]\n",
      " [0.56694293]\n",
      " [0.56669974]\n",
      " [0.7302792 ]\n",
      " [0.49219072]\n",
      " [0.57837296]\n",
      " [0.8830825 ]\n",
      " [0.5704645 ]\n",
      " [0.5585593 ]\n",
      " [0.72587335]\n",
      " [0.6340435 ]\n",
      " [0.6814599 ]\n",
      " [0.7983798 ]\n",
      " [0.47779986]\n",
      " [0.6372019 ]\n",
      " [0.5179228 ]\n",
      " [0.7367841 ]\n",
      " [0.80057454]\n",
      " [0.81147313]\n",
      " [0.3652671 ]\n",
      " [0.7916233 ]\n",
      " [0.6382241 ]\n",
      " [0.5762019 ]\n",
      " [0.48852298]\n",
      " [0.5992434 ]\n",
      " [0.701759  ]\n",
      " [0.6800525 ]\n",
      " [0.8026967 ]\n",
      " [0.4548727 ]\n",
      " [0.7905897 ]\n",
      " [0.80188614]\n",
      " [0.37453178]\n",
      " [0.6214391 ]\n",
      " [0.7154435 ]\n",
      " [0.3845271 ]\n",
      " [0.4106272 ]\n",
      " [0.6746813 ]\n",
      " [0.7119819 ]\n",
      " [0.80347025]\n",
      " [0.6561362 ]\n",
      " [0.7348298 ]\n",
      " [0.5028045 ]\n",
      " [0.6142848 ]\n",
      " [0.7929572 ]\n",
      " [0.79699624]\n",
      " [0.7582365 ]\n",
      " [0.7330463 ]\n",
      " [0.63538665]\n",
      " [0.8873724 ]\n",
      " [0.7684572 ]\n",
      " [0.7008762 ]\n",
      " [0.51776946]\n",
      " [0.53056437]\n",
      " [0.6731758 ]\n",
      " [0.6422986 ]\n",
      " [0.26874265]\n",
      " [0.44041428]\n",
      " [0.6211189 ]\n",
      " [0.7044734 ]\n",
      " [0.53800297]\n",
      " [0.60870117]\n",
      " [0.6947745 ]\n",
      " [0.7027912 ]\n",
      " [0.8926649 ]\n",
      " [0.7809198 ]\n",
      " [0.5605907 ]\n",
      " [0.18347695]\n",
      " [0.40618294]\n",
      " [0.7225615 ]\n",
      " [0.6704781 ]\n",
      " [0.5274234 ]\n",
      " [0.5042921 ]\n",
      " [0.7556198 ]\n",
      " [0.66915864]\n",
      " [0.4798616 ]\n",
      " [0.40391853]\n",
      " [0.5853695 ]\n",
      " [0.87181056]\n",
      " [0.7498048 ]\n",
      " [0.66048145]\n",
      " [0.7051263 ]\n",
      " [0.77566075]\n",
      " [0.72712076]\n",
      " [0.69337547]\n",
      " [0.6105713 ]\n",
      " [0.61809504]\n",
      " [0.74098915]\n",
      " [0.43273634]\n",
      " [0.82733846]\n",
      " [0.7348905 ]\n",
      " [0.45716506]\n",
      " [0.53743076]\n",
      " [0.6790386 ]\n",
      " [0.7098943 ]\n",
      " [0.84452015]\n",
      " [0.5113604 ]\n",
      " [0.809181  ]\n",
      " [0.4759887 ]\n",
      " [0.87775934]\n",
      " [0.5816973 ]\n",
      " [0.59831655]\n",
      " [0.73220354]\n",
      " [0.81131387]\n",
      " [0.29757136]\n",
      " [0.47043037]\n",
      " [0.6804256 ]\n",
      " [0.64379215]\n",
      " [0.44020775]\n",
      " [0.61380655]\n",
      " [0.54844636]\n",
      " [0.5622736 ]\n",
      " [0.7802887 ]\n",
      " [0.58717304]\n",
      " [0.83344054]\n",
      " [0.6756706 ]\n",
      " [0.46735495]\n",
      " [0.7574985 ]\n",
      " [0.5541413 ]\n",
      " [0.81465614]\n",
      " [0.43330538]\n",
      " [0.40304378]\n",
      " [0.7914939 ]\n",
      " [0.45013455]\n",
      " [0.4185415 ]\n",
      " [0.74886334]\n",
      " [0.8462277 ]\n",
      " [0.7448448 ]\n",
      " [0.77787733]\n",
      " [0.6970291 ]\n",
      " [0.8397244 ]\n",
      " [0.6051235 ]\n",
      " [0.6424073 ]\n",
      " [0.5920842 ]\n",
      " [0.91368544]\n",
      " [0.61300343]\n",
      " [0.43566516]\n",
      " [0.73545814]\n",
      " [0.6818993 ]\n",
      " [0.64522594]\n",
      " [0.8376848 ]\n",
      " [0.02831894]\n",
      " [0.73157185]\n",
      " [0.5989935 ]\n",
      " [0.6042358 ]\n",
      " [0.6580559 ]\n",
      " [0.84198695]\n",
      " [0.6400293 ]\n",
      " [0.71249396]\n",
      " [0.732773  ]\n",
      " [0.6820368 ]\n",
      " [0.6039207 ]\n",
      " [0.6776564 ]\n",
      " [0.7414209 ]\n",
      " [0.5676751 ]\n",
      " [0.6325666 ]\n",
      " [0.87162566]\n",
      " [0.6434225 ]\n",
      " [0.85667723]\n",
      " [0.58110785]\n",
      " [0.7470507 ]\n",
      " [0.8490195 ]\n",
      " [0.6166775 ]\n",
      " [0.75214326]\n",
      " [0.3237257 ]\n",
      " [0.5272364 ]\n",
      " [0.6996714 ]\n",
      " [0.7560638 ]\n",
      " [0.64495915]\n",
      " [0.77658916]\n",
      " [0.7451259 ]\n",
      " [0.5703789 ]\n",
      " [0.71036255]\n",
      " [0.62139106]\n",
      " [0.71933496]\n",
      " [0.58213484]\n",
      " [0.699684  ]\n",
      " [0.7926363 ]\n",
      " [0.7060022 ]\n",
      " [0.4997322 ]\n",
      " [0.5233385 ]\n",
      " [0.6439638 ]\n",
      " [0.26150724]\n",
      " [0.87067306]\n",
      " [0.41715223]\n",
      " [0.6825344 ]\n",
      " [0.81877965]\n",
      " [0.82221854]\n",
      " [0.5479759 ]\n",
      " [0.83804274]\n",
      " [0.58443403]\n",
      " [0.77256083]\n",
      " [0.76932013]\n",
      " [0.4393624 ]\n",
      " [0.45420927]\n",
      " [0.68048024]\n",
      " [0.8339385 ]\n",
      " [0.70505095]\n",
      " [0.66574436]\n",
      " [0.78540003]\n",
      " [0.8085352 ]\n",
      " [0.34040347]\n",
      " [0.6480969 ]\n",
      " [0.7228552 ]\n",
      " [0.7220858 ]\n",
      " [0.85517514]\n",
      " [0.7542764 ]\n",
      " [0.8400842 ]\n",
      " [0.7968956 ]\n",
      " [0.8117808 ]\n",
      " [0.53570867]\n",
      " [0.44072884]\n",
      " [0.845289  ]\n",
      " [0.7728878 ]\n",
      " [0.82152504]\n",
      " [0.59268355]\n",
      " [0.7495269 ]\n",
      " [0.6011267 ]\n",
      " [0.741137  ]\n",
      " [0.9025897 ]\n",
      " [0.82287455]\n",
      " [0.7559706 ]\n",
      " [0.6555704 ]\n",
      " [0.7252413 ]\n",
      " [0.6394199 ]\n",
      " [0.5878141 ]\n",
      " [0.49704257]\n",
      " [0.78916436]\n",
      " [0.6259918 ]\n",
      " [0.68087345]\n",
      " [0.74533516]\n",
      " [0.87412035]\n",
      " [0.4806242 ]\n",
      " [0.35235247]\n",
      " [0.6050108 ]\n",
      " [0.6186249 ]\n",
      " [0.7384173 ]\n",
      " [0.7001738 ]\n",
      " [0.7128949 ]\n",
      " [0.34282333]\n",
      " [0.25251898]\n",
      " [0.68273   ]\n",
      " [0.37458664]\n",
      " [0.39436623]\n",
      " [0.827482  ]\n",
      " [0.71529984]\n",
      " [0.73479176]\n",
      " [0.74662286]\n",
      " [0.7456601 ]\n",
      " [0.72093385]\n",
      " [0.7910212 ]\n",
      " [0.7639601 ]\n",
      " [0.7449987 ]\n",
      " [0.8031191 ]\n",
      " [0.70720416]\n",
      " [0.3291618 ]\n",
      " [0.6807388 ]\n",
      " [0.7385482 ]\n",
      " [0.724848  ]\n",
      " [0.7592449 ]\n",
      " [0.62186265]\n",
      " [0.7800548 ]\n",
      " [0.5218071 ]\n",
      " [0.66874164]\n",
      " [0.8285042 ]\n",
      " [0.69283986]\n",
      " [0.65411   ]\n",
      " [0.77820504]\n",
      " [0.53159964]\n",
      " [0.59837806]\n",
      " [0.7376119 ]\n",
      " [0.57465273]\n",
      " [0.70179987]\n",
      " [0.21090889]\n",
      " [0.49407625]\n",
      " [0.8543018 ]\n",
      " [0.8212297 ]\n",
      " [0.7407628 ]\n",
      " [0.79723626]\n",
      " [0.90456784]\n",
      " [0.6519023 ]\n",
      " [0.8050195 ]\n",
      " [0.28325665]\n",
      " [0.8716329 ]\n",
      " [0.5441981 ]\n",
      " [0.5639199 ]\n",
      " [0.624195  ]\n",
      " [0.62473565]\n",
      " [0.5327781 ]\n",
      " [0.37723958]\n",
      " [0.800215  ]\n",
      " [0.81449926]\n",
      " [0.52811086]\n",
      " [0.80813146]\n",
      " [0.81371474]\n",
      " [0.8364954 ]\n",
      " [0.7860317 ]\n",
      " [0.4335675 ]\n",
      " [0.54568547]\n",
      " [0.7349688 ]\n",
      " [0.23235595]\n",
      " [0.7729209 ]\n",
      " [0.47278392]\n",
      " [0.8266682 ]\n",
      " [0.772998  ]\n",
      " [0.48482257]\n",
      " [0.34642833]\n",
      " [0.71362007]\n",
      " [0.5217325 ]\n",
      " [0.7897998 ]\n",
      " [0.7050545 ]\n",
      " [0.86828285]\n",
      " [0.6643749 ]\n",
      " [0.74356174]\n",
      " [0.58275676]\n",
      " [0.71257234]\n",
      " [0.1973004 ]\n",
      " [0.6711064 ]\n",
      " [0.7909037 ]\n",
      " [0.56737816]\n",
      " [0.75171244]\n",
      " [0.6367635 ]\n",
      " [0.67881846]\n",
      " [0.895376  ]\n",
      " [0.69477165]\n",
      " [0.6248235 ]\n",
      " [0.7768446 ]\n",
      " [0.72474957]\n",
      " [0.8352172 ]\n",
      " [0.72523534]\n",
      " [0.79005516]\n",
      " [0.6845854 ]\n",
      " [0.5297705 ]\n",
      " [0.8134089 ]\n",
      " [0.6548021 ]\n",
      " [0.6676531 ]\n",
      " [0.6376958 ]\n",
      " [0.7879865 ]\n",
      " [0.66695845]\n",
      " [0.6248885 ]\n",
      " [0.55970025]\n",
      " [0.38869932]\n",
      " [0.5637348 ]\n",
      " [0.8614235 ]\n",
      " [0.81378776]\n",
      " [0.66151625]\n",
      " [0.5540633 ]\n",
      " [0.7452468 ]\n",
      " [0.7202868 ]\n",
      " [0.73474115]\n",
      " [0.774703  ]\n",
      " [0.3947448 ]\n",
      " [0.5419957 ]\n",
      " [0.5793453 ]\n",
      " [0.20068568]\n",
      " [0.3645419 ]\n",
      " [0.36502394]\n",
      " [0.77656335]\n",
      " [0.59849954]\n",
      " [0.7374202 ]\n",
      " [0.8353451 ]\n",
      " [0.7322906 ]\n",
      " [0.5209058 ]\n",
      " [0.45492265]\n",
      " [0.8841945 ]\n",
      " [0.7182431 ]\n",
      " [0.33591124]\n",
      " [0.7230232 ]\n",
      " [0.434219  ]\n",
      " [0.60754424]\n",
      " [0.7872021 ]\n",
      " [0.7459206 ]\n",
      " [0.8151507 ]\n",
      " [0.85499567]\n",
      " [0.46759748]\n",
      " [0.6073117 ]\n",
      " [0.6290368 ]\n",
      " [0.5749744 ]\n",
      " [0.6019021 ]\n",
      " [0.73307633]\n",
      " [0.8808275 ]\n",
      " [0.56267375]\n",
      " [0.40806106]\n",
      " [0.6787434 ]\n",
      " [0.40532017]\n",
      " [0.4670154 ]\n",
      " [0.47562888]\n",
      " [0.8532585 ]\n",
      " [0.7468824 ]\n",
      " [0.52162945]\n",
      " [0.81392956]\n",
      " [0.60106474]\n",
      " [0.5331684 ]\n",
      " [0.61985743]\n",
      " [0.7279837 ]\n",
      " [0.46219558]\n",
      " [0.6693693 ]\n",
      " [0.7400633 ]\n",
      " [0.81938314]\n",
      " [0.73228586]\n",
      " [0.79282224]\n",
      " [0.5571834 ]\n",
      " [0.66368985]\n",
      " [0.7152815 ]\n",
      " [0.56145483]\n",
      " [0.66498244]\n",
      " [0.7560071 ]\n",
      " [0.7933072 ]\n",
      " [0.45118827]\n",
      " [0.29749745]\n",
      " [0.86011   ]\n",
      " [0.63688964]\n",
      " [0.80973613]\n",
      " [0.8679766 ]\n",
      " [0.66906124]\n",
      " [0.5692027 ]\n",
      " [0.5814649 ]\n",
      " [0.7021421 ]\n",
      " [0.7252511 ]\n",
      " [0.61717904]\n",
      " [0.66151714]\n",
      " [0.5223927 ]\n",
      " [0.8412026 ]\n",
      " [0.62022936]\n",
      " [0.6947666 ]\n",
      " [0.24805325]\n",
      " [0.7447666 ]\n",
      " [0.76200646]\n",
      " [0.8301761 ]\n",
      " [0.5908929 ]\n",
      " [0.87334025]\n",
      " [0.75665295]\n",
      " [0.728519  ]\n",
      " [0.44939935]\n",
      " [0.7672452 ]\n",
      " [0.77422523]\n",
      " [0.500501  ]\n",
      " [0.25134236]\n",
      " [0.64010006]\n",
      " [0.47760952]\n",
      " [0.47368267]\n",
      " [0.42209595]\n",
      " [0.65537477]\n",
      " [0.49556348]\n",
      " [0.53146166]\n",
      " [0.857248  ]\n",
      " [0.4450275 ]\n",
      " [0.61489636]\n",
      " [0.37110206]\n",
      " [0.6247028 ]\n",
      " [0.68154305]\n",
      " [0.7311458 ]\n",
      " [0.3981518 ]\n",
      " [0.68930703]\n",
      " [0.49872598]\n",
      " [0.8566052 ]\n",
      " [0.67995924]\n",
      " [0.7485853 ]\n",
      " [0.7004728 ]\n",
      " [0.72386414]\n",
      " [0.83962345]], Y_pred : [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], acc_val : 0.6001031398773193\n"
     ]
    }
   ],
   "source": [
    "n_steps = 2001\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(n_steps):\n",
    "        cost_val , _ = sess.run([cost, train], feed_dict = {X:X_data, Y : Y_data})\n",
    "        if(step%200==0):\n",
    "            print(step , \":\", cost_val)\n",
    "    Y_proba, Y_pred ,acc_val = sess.run([hypothesis, predicted, accuracy], feed_dict ={X:X_data, Y: Y_data})\n",
    "    print(\"Y_proba : {}, Y_pred : {}, acc_val : {}\".format(Y_proba, Y_pred, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6장 softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "Y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X_test = [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_classes = 3\n",
    "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,n_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([n_classes]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost  = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis = 1))\n",
    "\n",
    "predict = tf.argmax(hypothesis, 1)\n",
    "training_op = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.737251\n",
      "200 1.1762998\n",
      "400 0.84256655\n",
      "600 0.69685197\n",
      "800 0.6076567\n",
      "1000 0.5503291\n",
      "1200 0.5119897\n",
      "1400 0.4846735\n",
      "1600 0.46384206\n",
      "1800 0.4470445\n",
      "2000 0.4329281\n",
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(2001):\n",
    "        cost_val, _ = sess.run([cost, training_op], feed_dict = {X: X_data, Y:Y_data})\n",
    "        if(step%200 ==0):\n",
    "            print(step , cost_val )\n",
    "    Y_pred = sess.run(predict, feed_dict = {X:X_test})\n",
    "    print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6장 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "Y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X_test = [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_classes = 3\n",
    "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,n_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([n_classes]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "xentropy  = tf.nn.softmax_cross_entropy_with_logits(logits = tf.matmul(X,W) +b,labels = Y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "\n",
    "predict = tf.argmax(hypothesis, 1)\n",
    "training_op = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.737251\n",
      "200 1.1762997\n",
      "400 0.84256643\n",
      "600 0.69685197\n",
      "800 0.6076568\n",
      "1000 0.5503292\n",
      "1200 0.5119897\n",
      "1400 0.48467353\n",
      "1600 0.46384206\n",
      "1800 0.4470445\n",
      "2000 0.43292817\n",
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(2001):\n",
    "        cost_val, _ = sess.run([loss, training_op], feed_dict = {X: X_data, Y:Y_data})\n",
    "        if(step%200 ==0):\n",
    "            print(step , cost_val )\n",
    "    Y_pred = sess.run(predict, feed_dict = {X:X_test})\n",
    "    print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zoo 데이터 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/hunkim/DeepLearningZeroToAll/master/data-04-zoo.csv'\n",
    "path = './datasets/'\n",
    "name = 'zoo.csv'\n",
    "download_data(url,path, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "XY = np.loadtxt('./datasets/zoo.csv', delimiter=',',dtype = np.float32)\n",
    "X_train = XY[:,:-1]\n",
    "Y_train = XY[:, -1]\n",
    "\n",
    "n_classes = 7\n",
    "attribute = 16\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, attribute])\n",
    "Y = tf.placeholder(tf.int32, shape = [None])\n",
    "Y_one_hot= tf.one_hot(Y, n_classes)\n",
    "Y_one_hot= tf.reshape(Y_one_hot, [-1,n_classes])\n",
    "W = tf.Variable(tf.random_normal([attribute, n_classes]), name = 'weight')\n",
    "b= tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "logits = tf.matmul(X,W)+b\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels= Y_one_hot)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, Y,1)\n",
    "accuracy= tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "predict = tf.argmax(logits,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  1.0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 1, 예측값 : 1\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(2001):\n",
    "        sess.run(training_op , feed_dict = {X: X_train, Y: Y_train})\n",
    "        if(step % 200 ==0):\n",
    "            loss_val = sess.run([loss], feed_dict = {X:X_train, Y: Y_train})\n",
    "    acc_val  =sess.run(accuracy, feed_dict = {X:X_train, Y: Y_train})\n",
    "    Y_pred = sess.run(predict, feed_dict = {X:X_train})\n",
    "    print(\"accuracy: \",acc_val)\n",
    "    for pred, test in zip(Y_pred,Y_train.flatten()):\n",
    "        print('실제값 : {}, 예측값 : {}'.format(int(test),pred))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.nn.softmax 안쓰고 tf.nn.sparse로 바꾼것 (원 핫 인코딩을 할 필요 없다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "XY = np.loadtxt('./datasets/zoo.csv', delimiter=',',dtype = np.float32)\n",
    "X_train = XY[:,:-1]\n",
    "Y_train = XY[:, -1]\n",
    "\n",
    "n_classes = 7\n",
    "attribute = 16\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, attribute])\n",
    "Y = tf.placeholder(tf.int32, shape = [None])\n",
    "Y_one_hot= tf.one_hot(Y, n_classes)\n",
    "Y_one_hot= tf.reshape(Y_one_hot, [-1,n_classes])\n",
    "W = tf.Variable(tf.random_normal([attribute, n_classes]), name = 'weight')\n",
    "b= tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "logits = tf.matmul(X,W)+b\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels= Y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, Y,1)\n",
    "accuracy= tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "predict = tf.argmax(logits,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  1.0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 4, 예측값 : 4\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 2, 예측값 : 2\n",
      "실제값 : 3, 예측값 : 3\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 1, 예측값 : 1\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 5, 예측값 : 5\n",
      "실제값 : 0, 예측값 : 0\n",
      "실제값 : 6, 예측값 : 6\n",
      "실제값 : 1, 예측값 : 1\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(2001):\n",
    "        sess.run(training_op , feed_dict = {X: X_train, Y: Y_train})\n",
    "        if(step % 200 ==0):\n",
    "            loss_val,log = sess.run([loss,logits], feed_dict = {X:X_train, Y: Y_train})\n",
    "    acc_val  =sess.run(accuracy, feed_dict = {X:X_train, Y: Y_train})\n",
    "    Y_pred = sess.run(predict, feed_dict = {X:X_train})\n",
    "    print(\"accuracy: \",acc_val)\n",
    "    for pred, test in zip(Y_pred,Y_train.flatten()):\n",
    "        print('실제값 : {}, 예측값 : {}'.format(int(test),pred))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
