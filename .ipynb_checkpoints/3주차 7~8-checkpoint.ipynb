{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "Y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "X_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "Y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None,3])\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.nn.softmax(tf.matmul(X,W))\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(logits), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(logits,1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y,axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, loss : 7.386537551879883, W : [[ 1.3475733  -1.2905289   1.7472453 ]\n",
      " [ 0.46938467 -0.5313217   0.25201392]\n",
      " [ 0.37612063 -0.920832    0.54548866]]\n",
      "step : 1, loss : 5.894815444946289, W : [[ 1.3271755  -1.253046    1.7301602 ]\n",
      " [ 0.4235289  -0.35635766  0.12290563]\n",
      " [ 0.3398038  -0.73335123  0.39432472]]\n",
      "step : 2, loss : 4.889636516571045, W : [[ 1.2869463  -1.21559     1.7329334 ]\n",
      " [ 0.2779017  -0.18145297  0.09362815]\n",
      " [ 0.20329845 -0.5459058   0.3433847 ]]\n",
      "step : 3, loss : 3.8777670860290527, W : [[ 1.2681692  -1.1782118   1.7143323 ]\n",
      " [ 0.24029842 -0.00673559 -0.04348592]\n",
      " [ 0.17366639 -0.35858974  0.1857007 ]]\n",
      "step : 4, loss : 2.879331111907959, W : [[ 1.2267766  -1.1411157   1.7186288 ]\n",
      " [ 0.08879347  0.16716452 -0.06588105]\n",
      " [ 0.02973826 -0.1719689   0.14300801]]\n",
      "step : 5, loss : 1.890010952949524, W : [[ 1.2104155  -1.1059872   1.6998613 ]\n",
      " [ 0.06298157  0.33208913 -0.20499372]\n",
      " [ 0.0100564   0.00633733 -0.01561636]]\n",
      "step : 6, loss : 1.1454370021820068, W : [[ 1.1822742  -1.0880804   1.7100958 ]\n",
      " [-0.01139517  0.40040916 -0.19893704]\n",
      " [-0.06372551  0.09361682 -0.02911394]]\n",
      "step : 7, loss : 1.1013025045394897, W : [[ 1.1783826  -1.0927684   1.7186754 ]\n",
      " [ 0.04303167  0.34042665 -0.19338137]\n",
      " [-0.01226021  0.05836009 -0.0453225 ]]\n",
      "step : 8, loss : 1.0699759721755981, W : [[ 1.1586907  -1.0831769   1.7287757 ]\n",
      " [ 0.01495583  0.36125985 -0.18613872]\n",
      " [-0.04184154  0.09994009 -0.05732117]]\n",
      "step : 9, loss : 1.0575780868530273, W : [[ 1.1490825  -1.0831853   1.7383922 ]\n",
      " [ 0.04025291  0.328093   -0.17826895]\n",
      " [-0.01946341  0.08943468 -0.0691939 ]]\n",
      "step : 10, loss : 1.0469529628753662, W : [[ 1.1322222  -1.0764805   1.7485479 ]\n",
      " [ 0.02691866  0.33326954 -0.17011124]\n",
      " [-0.03489513  0.11517872 -0.0795062 ]]\n",
      "step : 11, loss : 1.038614273071289, W : [[ 1.1206499  -1.0746875   1.7583271 ]\n",
      " [ 0.04120185  0.3111175  -0.16224238]\n",
      " [-0.02335211  0.11424354 -0.09011403]]\n",
      "step : 12, loss : 1.0309391021728516, W : [[ 1.1053188  -1.0693566   1.7683272 ]\n",
      " [ 0.0351601   0.3093591  -0.15444225]\n",
      " [-0.03167645  0.13241206 -0.09995821]]\n",
      "step : 13, loss : 1.0239101648330688, W : [[ 1.0928848  -1.0666995   1.7781042 ]\n",
      " [ 0.04401259  0.29297924 -0.1469149 ]\n",
      " [-0.0254151   0.13603957 -0.10984708]]\n",
      "step : 14, loss : 1.017235279083252, W : [[ 1.078443   -1.0621194   1.7879659 ]\n",
      " [ 0.04176747  0.2878687  -0.13955922]\n",
      " [-0.02998236  0.15004018 -0.11928043]]\n",
      "step : 15, loss : 1.010875940322876, W : [[ 1.0656428  -1.0590414   1.7976881 ]\n",
      " [ 0.04774834  0.2747691  -0.13244049]\n",
      " [-0.02646163  0.15585783 -0.12861879]]\n",
      "step : 16, loss : 1.0047621726989746, W : [[ 1.051761   -1.0549021   1.8074306 ]\n",
      " [ 0.04752289  0.26807728 -0.12552322]\n",
      " [-0.02897401  0.16738926 -0.13763784]]\n",
      "step : 17, loss : 0.9988738298416138, W : [[ 1.0388422  -1.0516303   1.8170776 ]\n",
      " [ 0.0519219   0.25697607 -0.11882101]\n",
      " [-0.02690777  0.17418425 -0.14649907]]\n",
      "step : 18, loss : 0.9931877851486206, W : [[ 1.0253453  -1.0477691   1.8267133 ]\n",
      " [ 0.05277725  0.24962355 -0.11232384]\n",
      " [-0.02825996  0.18414281 -0.15510543]]\n",
      "step : 19, loss : 0.9876928329467773, W : [[ 1.0124366  -1.0444238   1.8362768 ]\n",
      " [ 0.05626981  0.23983511 -0.10602795]\n",
      " [-0.02696903  0.19127616 -0.1635297 ]]\n",
      "step : 20, loss : 0.9823777675628662, W : [[ 0.9992291  -1.0407528   1.8458133 ]\n",
      " [ 0.05769753  0.23231082 -0.09993134]\n",
      " [-0.02764139  0.20015126 -0.17173243]]\n",
      "step : 21, loss : 0.9772347211837769, W : [[ 0.98639876 -1.0373994   1.8552902 ]\n",
      " [ 0.0606457   0.22345708 -0.09402578]\n",
      " [-0.02675751  0.20728202 -0.17974706]]\n",
      "step : 22, loss : 0.9722557067871094, W : [[ 0.97342694 -1.0338696   1.8647323 ]\n",
      " [ 0.06236485  0.21602283 -0.08831067]\n",
      " [-0.02701258  0.21534982 -0.1875598 ]]\n",
      "step : 23, loss : 0.9674339294433594, W : [[ 0.96071136 -1.0305436   1.8741218 ]\n",
      " [ 0.06496626  0.2078883  -0.08277754]\n",
      " [-0.02633209  0.22229627 -0.19518673]]\n",
      "step : 24, loss : 0.9627628922462463, W : [[ 0.94794405 -1.0271271   1.8834727 ]\n",
      " [ 0.06681884  0.20068349 -0.07742529]\n",
      " [-0.026316    0.22971871 -0.20262526]]\n",
      "step : 25, loss : 0.9582363963127136, W : [[ 0.93536216 -1.0238476   1.892775  ]\n",
      " [ 0.06918331  0.19314013 -0.07224641]\n",
      " [-0.02572452  0.23638597 -0.209884  ]]\n",
      "step : 26, loss : 0.9538483619689941, W : [[ 0.9227803  -1.0205276   1.902037  ]\n",
      " [ 0.07107904  0.18623683 -0.06723882]\n",
      " [-0.02552036  0.24326281 -0.216965  ]]\n",
      "step : 27, loss : 0.9495929479598999, W : [[ 0.9103415  -1.0173049   1.9112531 ]\n",
      " [ 0.07326873  0.17920443 -0.06239614]\n",
      " [-0.02495304  0.24960427 -0.22387378]]\n",
      "step : 28, loss : 0.9454647302627563, W : [[ 0.8979327  -1.014071    1.9204279 ]\n",
      " [ 0.07515553  0.17263702 -0.05771552]\n",
      " [-0.02460922  0.25600082 -0.23061416]]\n",
      "step : 29, loss : 0.9414582252502441, W : [[ 0.8856411  -1.01091     1.9295585 ]\n",
      " [ 0.07720651  0.16606197 -0.05319142]\n",
      " [-0.02402931  0.26199716 -0.23719037]]\n",
      "step : 30, loss : 0.9375680685043335, W : [[ 0.8733967  -1.0077549   1.9386477 ]\n",
      " [ 0.07905429  0.1598433  -0.04882054]\n",
      " [-0.02357506  0.26795936 -0.2436068 ]]\n",
      "step : 31, loss : 0.9337893724441528, W : [[ 0.8612532  -1.0046576   1.9476941 ]\n",
      " [ 0.0809882   0.15368682 -0.04459798]\n",
      " [-0.02296202  0.27360678 -0.24986725]]\n",
      "step : 32, loss : 0.9301172494888306, W : [[ 0.84916663 -1.0015762   1.9566991 ]\n",
      " [ 0.08277977  0.14781742 -0.04052016]\n",
      " [-0.02241579  0.27916938 -0.25597608]]\n",
      "step : 33, loss : 0.9265470504760742, W : [[ 0.8371703  -0.9985428   1.9656621 ]\n",
      " [ 0.08461024  0.14204946 -0.03658267]\n",
      " [-0.02175864  0.28447303 -0.2619369 ]]\n",
      "step : 34, loss : 0.9230743050575256, W : [[ 0.8252362  -0.9955308   1.9745842 ]\n",
      " [ 0.08633624  0.13652262 -0.03278183]\n",
      " [-0.02113279  0.2896642  -0.26775393]]\n",
      "step : 35, loss : 0.9196946024894714, W : [[ 0.813385   -0.9925603   1.983465  ]\n",
      " [ 0.08807248  0.13111813 -0.02911358]\n",
      " [-0.02042627  0.2946344  -0.27343065]]\n",
      "step : 36, loss : 0.9164038896560669, W : [[ 0.8015987  -0.98961425  1.9923052 ]\n",
      " [ 0.0897283   0.12592301 -0.02557428]\n",
      " [-0.01972969  0.29947823 -0.27897108]]\n",
      "step : 37, loss : 0.9131981730461121, W : [[ 0.7898901  -0.9867052   2.0011048 ]\n",
      " [ 0.09137705  0.12086012 -0.02216013]\n",
      " [-0.01897204  0.3041281  -0.28437862]]\n",
      "step : 38, loss : 0.9100736379623413, W : [[ 0.7782473  -0.98382187  2.0098643 ]\n",
      " [ 0.09296107  0.11598355 -0.01886758]\n",
      " [-0.01821163  0.3086461  -0.28965706]]\n",
      "step : 39, loss : 0.9070268869400024, W : [[ 0.7666782  -0.98097223  2.0185838 ]\n",
      " [ 0.09452774  0.11124234 -0.01569306]\n",
      " [-0.01740318  0.31299028 -0.2948097 ]]\n",
      "step : 40, loss : 0.9040542840957642, W : [[ 0.75517493 -0.97814876  2.0272636 ]\n",
      " [ 0.09604021  0.10666994 -0.01263312]\n",
      " [-0.01658464  0.31720212 -0.2998401 ]]\n",
      "step : 41, loss : 0.901152491569519, W : [[ 0.7437422  -0.97535634  2.035904  ]\n",
      " [ 0.09752949  0.10223197 -0.00968442]\n",
      " [-0.01572701  0.32125586 -0.30475146]]\n",
      "step : 42, loss : 0.8983184099197388, W : [[ 0.7323746  -0.9725899   2.0445051 ]\n",
      " [ 0.09897189  0.0979488  -0.00684365]\n",
      " [-0.01485534  0.32517985 -0.30954713]]\n",
      "step : 43, loss : 0.8955492377281189, W : [[ 0.72107494 -0.96985245  2.0530674 ]\n",
      " [ 0.10038801  0.0937967  -0.00410766]\n",
      " [-0.01395096  0.3289585  -0.31423017]]\n",
      "step : 44, loss : 0.8928419351577759, W : [[ 7.0983934e-01 -9.6714044e-01  2.0615911e+00]\n",
      " [ 1.0176259e-01  8.9787766e-02 -1.4733172e-03]\n",
      " [-1.3030645e-02  3.3261180e-01 -3.1880379e-01]]\n",
      "step : 45, loss : 0.89019376039505, W : [[ 6.98669374e-01 -9.64455605e-01  2.07007623e+00]\n",
      " [ 1.03109516e-01  8.59051347e-02  1.06239645e-03]\n",
      " [-1.20824268e-02  3.36130679e-01 -3.23270887e-01]]\n",
      "step : 46, loss : 0.8876023292541504, W : [[ 0.6875622  -0.96179545  2.0785232 ]\n",
      " [ 0.10441904  0.08215555  0.00350246]\n",
      " [-0.01111764  0.33952945 -0.32763445]]\n",
      "step : 47, loss : 0.8850651383399963, W : [[ 0.6765186  -0.9591609   2.0869322 ]\n",
      " [ 0.1057006   0.07852675  0.0058497 ]\n",
      " [-0.01012869  0.34280333 -0.3318973 ]]\n",
      "step : 48, loss : 0.8825798034667969, W : [[ 0.66553646 -0.9565502   2.0953035 ]\n",
      " [ 0.10694804  0.07502209  0.00810692]\n",
      " [-0.00912338  0.34596294 -0.3360622 ]]\n",
      "step : 49, loss : 0.8801441788673401, W : [[ 0.65461594 -0.9539637   2.1036375 ]\n",
      " [ 0.10816797  0.07163227  0.01027682]\n",
      " [-0.00809687  0.34900612 -0.34013188]]\n",
      "step : 50, loss : 0.8777561187744141, W : [[ 0.6437554  -0.9514      2.1119344 ]\n",
      " [ 0.10935649  0.06835853  0.01236204]\n",
      " [-0.00705472  0.35194105 -0.34410894]]\n",
      "step : 51, loss : 0.8754137754440308, W : [[ 0.6329547  -0.9488593   2.1201944 ]\n",
      " [ 0.11051838  0.06519359  0.0143651 ]\n",
      " [-0.00599388  0.35476723 -0.34799597]]\n",
      "step : 52, loss : 0.873115062713623, W : [[ 0.6222125  -0.9463404   2.1284177 ]\n",
      " [ 0.11165122  0.06213732  0.01628853]\n",
      " [-0.00491836  0.3574912  -0.35179543]]\n",
      "step : 53, loss : 0.8708583116531372, W : [[ 0.6115285  -0.94384325  2.1366045 ]\n",
      " [ 0.11275849  0.05918386  0.01813472]\n",
      " [-0.00382634  0.3601135  -0.35550976]]\n",
      "step : 54, loss : 0.8686419129371643, W : [[ 0.60090154 -0.941367    2.1447554 ]\n",
      " [ 0.11383885  0.0563322   0.01990603]\n",
      " [-0.00272075  0.36263943 -0.35914126]]\n",
      "step : 55, loss : 0.8664641380310059, W : [[ 5.9033114e-01 -9.3891138e-01  2.1528702e+00]\n",
      " [ 1.1489489e-01  5.3577490e-02  2.1604715e-02]\n",
      " [-1.6005734e-03  3.6507025e-01 -3.6269224e-01]]\n",
      "step : 56, loss : 0.8643234372138977, W : [[ 5.7981628e-01 -9.3647569e-01  2.1609492e+00]\n",
      " [ 1.1592590e-01  5.0918221e-02  2.3232995e-02]\n",
      " [-4.6801439e-04  3.6741039e-01 -3.6616492e-01]]\n",
      "step : 57, loss : 0.8622184991836548, W : [[ 5.6935650e-01 -9.3405968e-01  2.1689930e+00]\n",
      " [ 1.1693388e-01  4.8350230e-02  2.4793008e-02]\n",
      " [ 6.7740434e-04  3.6966151e-01 -3.6956143e-01]]\n",
      "step : 58, loss : 0.8601478338241577, W : [[ 5.5895084e-01 -9.3166262e-01  2.1770017e+00]\n",
      " [ 1.1791860e-01  4.5871705e-02  2.6286814e-02]\n",
      " [ 1.8340109e-03  3.7182739e-01 -3.7288392e-01]]\n",
      "step : 59, loss : 0.8581101298332214, W : [[ 0.5485987  -0.9292842   2.1849754 ]\n",
      " [ 0.11888163  0.04347904  0.02771644]\n",
      " [ 0.00300195  0.37390992 -0.3761344 ]]\n",
      "step : 60, loss : 0.85610431432724, W : [[ 0.5382993  -0.9269239   2.1929145 ]\n",
      " [ 0.11982298  0.04117028  0.02908384]\n",
      " [ 0.00417986  0.37591243 -0.3793148 ]]\n",
      "step : 61, loss : 0.8541290760040283, W : [[ 0.5280521  -0.9245813   2.200819  ]\n",
      " [ 0.12074399  0.03894221  0.03039091]\n",
      " [ 0.00536774  0.37783685 -0.38242713]]\n",
      "step : 62, loss : 0.8521833419799805, W : [[ 0.5178562  -0.9222559   2.2086895 ]\n",
      " [ 0.12164478  0.0367928   0.03163952]\n",
      " [ 0.00656441  0.3796862  -0.38547316]]\n",
      "step : 63, loss : 0.8502659797668457, W : [[ 0.5077111  -0.9199473   2.216526  ]\n",
      " [ 0.12252654  0.03471914  0.03283142]\n",
      " [ 0.0077698   0.38146242 -0.38845477]]\n",
      "step : 64, loss : 0.8483761548995972, W : [[ 0.49761605 -0.91765517  2.224329  ]\n",
      " [ 0.12338947  0.03271927  0.03396836]\n",
      " [ 0.00898289  0.38316828 -0.3913737 ]]\n",
      "step : 65, loss : 0.8465127944946289, W : [[ 0.48757046 -0.9153791   2.2320986 ]\n",
      " [ 0.12423459  0.03079049  0.03505202]\n",
      " [ 0.01020351  0.38480562 -0.39423165]]\n",
      "step : 66, loss : 0.8446749448776245, W : [[ 0.47757366 -0.9131186   2.2398348 ]\n",
      " [ 0.12506218  0.02893088  0.03608401]\n",
      " [ 0.01143082  0.38637698 -0.39703032]]\n",
      "step : 67, loss : 0.8428619503974915, W : [[ 0.46762505 -0.9108734   2.247538  ]\n",
      " [ 0.12587316  0.02713798  0.03706595]\n",
      " [ 0.01266461  0.38788417 -0.39977127]]\n",
      "step : 68, loss : 0.8410727977752686, W : [[ 0.457724   -0.90864307  2.2552087 ]\n",
      " [ 0.12666781  0.02540994  0.03799935]\n",
      " [ 0.0139041   0.38932952 -0.4024561 ]]\n",
      "step : 69, loss : 0.8393068313598633, W : [[ 0.44786993 -0.90642726  2.262847  ]\n",
      " [ 0.12744695  0.02374447  0.03888571]\n",
      " [ 0.01514907  0.3907148  -0.40508634]]\n",
      "step : 70, loss : 0.8375632762908936, W : [[ 0.43806222 -0.9042256   2.270453  ]\n",
      " [ 0.12821092  0.02213977  0.03972645]\n",
      " [ 0.01639889  0.3920421  -0.40766346]]\n",
      "step : 71, loss : 0.8358414173126221, W : [[ 0.42830032 -0.9020378   2.278027  ]\n",
      " [ 0.12896039  0.02059377  0.04052299]\n",
      " [ 0.01765328  0.3933132  -0.41018894]]\n",
      "step : 72, loss : 0.8341405391693115, W : [[ 0.41858363 -0.8998635   2.2855694 ]\n",
      " [ 0.12969577  0.01910469  0.04127668]\n",
      " [ 0.01891171  0.39452994 -0.41266412]]\n",
      "step : 73, loss : 0.8324600458145142, W : [[ 0.40891162 -0.89770234  2.2930803 ]\n",
      " [ 0.13041763  0.01767068  0.04198883]\n",
      " [ 0.02017387  0.39569405 -0.41509038]]\n",
      "step : 74, loss : 0.8307994604110718, W : [[ 0.3992837  -0.89555407  2.30056   ]\n",
      " [ 0.1311264   0.01629001  0.04266073]\n",
      " [ 0.02143933  0.39680728 -0.41746905]]\n",
      "step : 75, loss : 0.8291580677032471, W : [[ 0.38969937 -0.8934184   2.3080087 ]\n",
      " [ 0.13182256  0.01496096  0.04329364]\n",
      " [ 0.02270775  0.39787123 -0.4198014 ]]\n",
      "step : 76, loss : 0.8275352716445923, W : [[ 0.3801581  -0.89129496  2.3154266 ]\n",
      " [ 0.13250653  0.01368189  0.04388873]\n",
      " [ 0.02397878  0.3988875  -0.4220887 ]]\n",
      "step : 77, loss : 0.8259307146072388, W : [[ 0.37065935 -0.8891836   2.322814  ]\n",
      " [ 0.13317876  0.01245121  0.04444718]\n",
      " [ 0.02525211  0.3998576  -0.42433214]]\n",
      "step : 78, loss : 0.8243436813354492, W : [[ 0.36120263 -0.8870839   2.330171  ]\n",
      " [ 0.13383967  0.01126739  0.04497011]\n",
      " [ 0.02652742  0.4007831  -0.42653292]]\n",
      "step : 79, loss : 0.8227739334106445, W : [[ 0.35178742 -0.88499564  2.337498  ]\n",
      " [ 0.13448963  0.0101289   0.04545863]\n",
      " [ 0.02780439  0.40166536 -0.42869216]]\n",
      "step : 80, loss : 0.821220874786377, W : [[ 0.34241325 -0.8829186   2.344795  ]\n",
      " [ 0.13512902  0.00903432  0.04591383]\n",
      " [ 0.02908274  0.40250584 -0.430811  ]]\n",
      "step : 81, loss : 0.8196839690208435, W : [[ 0.33307964 -0.88085246  2.3520625 ]\n",
      " [ 0.13575825  0.00798219  0.04633671]\n",
      " [ 0.03036223  0.40330583 -0.43289047]]\n",
      "step : 82, loss : 0.8181630969047546, W : [[ 0.32378608 -0.878797    2.3593006 ]\n",
      " [ 0.13637762  0.00697123  0.04672829]\n",
      " [ 0.03164254  0.40406668 -0.43493167]]\n",
      "step : 83, loss : 0.8166576027870178, W : [[ 0.31453213 -0.876752    2.3665094 ]\n",
      " [ 0.13698754  0.00600007  0.04708952]\n",
      " [ 0.03292352  0.40478966 -0.4369356 ]]\n",
      "step : 84, loss : 0.8151673078536987, W : [[ 0.30531734 -0.87471724  2.3736894 ]\n",
      " [ 0.13758825  0.00506749  0.04742139]\n",
      " [ 0.03420483  0.40547603 -0.43890327]]\n",
      "step : 85, loss : 0.8136916160583496, W : [[ 0.29614127 -0.8726924   2.3808408 ]\n",
      " [ 0.13818018  0.00417215  0.04772479]\n",
      " [ 0.03548636  0.4061269  -0.44083565]]\n",
      "step : 86, loss : 0.8122302889823914, W : [[ 0.28700346 -0.87067735  2.3879635 ]\n",
      " [ 0.13876352  0.003313    0.0480006 ]\n",
      " [ 0.03676778  0.4067435  -0.44273368]]\n",
      "step : 87, loss : 0.8107830286026001, W : [[ 0.27790347 -0.86867183  2.395058  ]\n",
      " [ 0.13933866  0.00248875  0.04824972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.03804901  0.40732685 -0.44459826]]\n",
      "step : 88, loss : 0.809349536895752, W : [[ 2.6884088e-01 -8.6667567e-01  2.4021244e+00]\n",
      " [ 1.3990580e-01  1.6983692e-03  4.8472982e-02]\n",
      " [ 3.9329764e-02  4.0787813e-01 -4.4643027e-01]]\n",
      "step : 89, loss : 0.8079293370246887, W : [[ 2.5981531e-01 -8.6468863e-01  2.4091630e+00]\n",
      " [ 1.4046526e-01  9.4072410e-04  4.8671179e-02]\n",
      " [ 4.0609930e-02  4.0839830e-01 -4.4823059e-01]]\n",
      "step : 90, loss : 0.8065222501754761, W : [[ 2.5082630e-01 -8.6271060e-01  2.4161739e+00]\n",
      " [ 1.4101727e-01  2.1478051e-04  4.8845109e-02]\n",
      " [ 4.1889295e-02  4.0888840e-01 -4.5000008e-01]]\n",
      "step : 91, loss : 0.8051280975341797, W : [[ 2.4187349e-01 -8.6074132e-01  2.4231575e+00]\n",
      " [ 1.4156210e-01 -4.8048532e-04  4.8995543e-02]\n",
      " [ 4.3167721e-02  4.0934941e-01 -4.5173952e-01]]\n",
      "step : 92, loss : 0.8037464022636414, W : [[ 2.3295647e-01 -8.5878056e-01  2.4301138e+00]\n",
      " [ 1.4209999e-01 -1.1460464e-03  4.9123228e-02]\n",
      " [ 4.4445060e-02  4.0978229e-01 -4.5344973e-01]]\n",
      "step : 93, loss : 0.8023770451545715, W : [[ 2.2407484e-01 -8.5682821e-01  2.4370432e+00]\n",
      " [ 1.4263114e-01 -1.7828520e-03  4.9228888e-02]\n",
      " [ 4.5721155e-02  4.1018796e-01 -4.5513147e-01]]\n",
      "step : 94, loss : 0.8010197281837463, W : [[ 2.1522824e-01 -8.5488409e-01  2.4439456e+00]\n",
      " [ 1.4315584e-01 -2.3918797e-03  4.9313225e-02]\n",
      " [ 4.6995912e-02  4.1056725e-01 -4.5678550e-01]]\n",
      "step : 95, loss : 0.7996741533279419, W : [[ 0.20641628 -0.852948    2.4508216 ]\n",
      " [ 0.1436742  -0.00297394  0.04937693]\n",
      " [ 0.04826912  0.4109211  -0.45841256]]\n",
      "step : 96, loss : 0.7983402609825134, W : [[ 0.1976386  -0.85101986  2.4576712 ]\n",
      " [ 0.14418653 -0.00352998  0.04942065]\n",
      " [ 0.04954077  0.41125026 -0.46001336]]\n",
      "step : 97, loss : 0.7970175743103027, W : [[ 0.18889484 -0.8490994   2.4644945 ]\n",
      " [ 0.14469291 -0.00406075  0.04944505]\n",
      " [ 0.05081064  0.41155562 -0.46158856]]\n",
      "step : 98, loss : 0.795706033706665, W : [[ 0.18018463 -0.84718657  2.4712918 ]\n",
      " [ 0.14519362 -0.00456717  0.04945075]\n",
      " [ 0.05207872  0.41183785 -0.46313888]]\n",
      "step : 99, loss : 0.7944055199623108, W : [[ 0.17150764 -0.8452811   2.4780633 ]\n",
      " [ 0.14568877 -0.00504993  0.04943835]\n",
      " [ 0.05334483  0.41209778 -0.46466494]]\n",
      "step : 100, loss : 0.7931156754493713, W : [[ 0.16286352 -0.84338295  2.4848094 ]\n",
      " [ 0.1461786  -0.00550984  0.04940843]\n",
      " [ 0.05460897  0.4123361  -0.46616742]]\n",
      "step : 101, loss : 0.7918362617492676, W : [[ 0.15425193 -0.84149194  2.49153   ]\n",
      " [ 0.14666322 -0.0059476   0.04936158]\n",
      " [ 0.05587099  0.41255358 -0.4676469 ]]\n",
      "step : 102, loss : 0.7905672788619995, W : [[ 0.14567254 -0.83960795  2.4982252 ]\n",
      " [ 0.1471428  -0.00636395  0.04929835]\n",
      " [ 0.05713082  0.41275087 -0.469104  ]]\n",
      "step : 103, loss : 0.7893083095550537, W : [[ 0.13712503 -0.8377308   2.5048957 ]\n",
      " [ 0.14761752 -0.0067596   0.04921929]\n",
      " [ 0.0583884   0.41292858 -0.4705393 ]]\n",
      "step : 104, loss : 0.7880594730377197, W : [[ 0.12860908 -0.83586043  2.5115414 ]\n",
      " [ 0.14808747 -0.00713518  0.0491249 ]\n",
      " [ 0.05964365  0.41308743 -0.4719534 ]]\n",
      "step : 105, loss : 0.7868203520774841, W : [[ 0.12012436 -0.83399665  2.5181623 ]\n",
      " [ 0.14855285 -0.00749136  0.0490157 ]\n",
      " [ 0.06089654  0.41322798 -0.47334683]]\n",
      "step : 106, loss : 0.7855907678604126, W : [[ 0.11167058 -0.8321394   2.5247588 ]\n",
      " [ 0.14901371 -0.00782873  0.04889221]\n",
      " [ 0.06214694  0.41335088 -0.47472015]]\n",
      "step : 107, loss : 0.7843708395957947, W : [[ 0.10324743 -0.83028847  2.531331  ]\n",
      " [ 0.14947028 -0.00814797  0.04875488]\n",
      " [ 0.0633949   0.41345665 -0.47607386]]\n",
      "step : 108, loss : 0.7831600904464722, W : [[ 0.09485459 -0.8284438   2.5378792 ]\n",
      " [ 0.14992256 -0.00844957  0.0486042 ]\n",
      " [ 0.06464024  0.41354594 -0.4774085 ]]\n",
      "step : 109, loss : 0.7819585800170898, W : [[ 0.0864918  -0.8266053   2.5444036 ]\n",
      " [ 0.1503708  -0.00873423  0.04844061]\n",
      " [ 0.06588305  0.4136192  -0.47872457]]\n",
      "step : 110, loss : 0.7807661294937134, W : [[ 0.07815874 -0.82477283  2.550904  ]\n",
      " [ 0.150815   -0.00900236  0.04826457]\n",
      " [ 0.06712317  0.41367704 -0.48002252]]\n",
      "step : 111, loss : 0.7795825004577637, W : [[ 0.06985514 -0.8229463   2.5573812 ]\n",
      " [ 0.15125534 -0.00925462  0.04807648]\n",
      " [ 0.06836066  0.4137199  -0.48130286]]\n",
      "step : 112, loss : 0.778407633304596, W : [[ 0.06158071 -0.82112557  2.563835  ]\n",
      " [ 0.15169185 -0.00949141  0.04787677]\n",
      " [ 0.0695954   0.41374835 -0.48256606]]\n",
      "step : 113, loss : 0.7772414684295654, W : [[ 0.05333518 -0.81931055  2.5702655 ]\n",
      " [ 0.15212472 -0.00971335  0.04766586]\n",
      " [ 0.07082745  0.41376278 -0.48381254]]\n",
      "step : 114, loss : 0.7760835886001587, W : [[ 0.04511828 -0.8175012   2.576673  ]\n",
      " [ 0.15255393 -0.0099208   0.04744411]\n",
      " [ 0.07205667  0.41376376 -0.48504275]]\n",
      "step : 115, loss : 0.7749341726303101, W : [[ 0.03692973 -0.8156974   2.5830576 ]\n",
      " [ 0.15297966 -0.01011435  0.04721193]\n",
      " [ 0.07328315  0.41375163 -0.4862571 ]]\n",
      "step : 116, loss : 0.7737929821014404, W : [[ 0.02876928 -0.813899    2.5894196 ]\n",
      " [ 0.15340194 -0.01029438  0.04696968]\n",
      " [ 0.07450679  0.41372693 -0.48745602]]\n",
      "step : 117, loss : 0.7726600170135498, W : [[ 0.02063667 -0.81210595  2.5957592 ]\n",
      " [ 0.15382089 -0.01046137  0.04671772]\n",
      " [ 0.07572761  0.41368997 -0.48863992]]\n",
      "step : 118, loss : 0.7715348601341248, W : [[ 0.01253164 -0.8103182   2.6020763 ]\n",
      " [ 0.15423653 -0.01061569  0.0464564 ]\n",
      " [ 0.07694555  0.41364127 -0.4898092 ]]\n",
      "step : 119, loss : 0.7704176902770996, W : [[ 0.00445394 -0.8085356   2.6083715 ]\n",
      " [ 0.15464902 -0.01075785  0.04618606]\n",
      " [ 0.07816069  0.41358113 -0.49096417]]\n",
      "step : 120, loss : 0.76930832862854, W : [[-0.00359668 -0.80675805  2.6146445 ]\n",
      " [ 0.15505834 -0.01088814  0.04590702]\n",
      " [ 0.07937291  0.41351002 -0.49210528]]\n",
      "step : 121, loss : 0.7682065963745117, W : [[-0.01162047 -0.8049855   2.6208959 ]\n",
      " [ 0.15546463 -0.01100703  0.04561961]\n",
      " [ 0.08058228  0.41342825 -0.49323285]]\n",
      "step : 122, loss : 0.7671124339103699, W : [[-0.01961765 -0.80321795  2.6271255 ]\n",
      " [ 0.15586792 -0.01111484  0.04532414]\n",
      " [ 0.08178874  0.41333622 -0.49434727]]\n",
      "step : 123, loss : 0.7660257816314697, W : [[-0.02758848 -0.80145526  2.6333337 ]\n",
      " [ 0.1562683  -0.011212    0.04502093]\n",
      " [ 0.08299234  0.4132342  -0.49544886]]\n",
      "step : 124, loss : 0.7649463415145874, W : [[-0.03553318 -0.79969734  2.6395204 ]\n",
      " [ 0.15666576 -0.01129878  0.04471026]\n",
      " [ 0.084193    0.41312262 -0.49653795]]\n",
      "step : 125, loss : 0.763874351978302, W : [[-0.04345198 -0.7979441   2.645686  ]\n",
      " [ 0.15706044 -0.0113756   0.04439242]\n",
      " [ 0.08539081  0.41300175 -0.49761486]]\n",
      "step : 126, loss : 0.7628093957901001, W : [[-0.05134511 -0.79619557  2.6518304 ]\n",
      " [ 0.15745232 -0.01144273  0.04406767]\n",
      " [ 0.08658569  0.41287196 -0.49867994]]\n",
      "step : 127, loss : 0.761751651763916, W : [[-0.0592128  -0.7944516   2.6579542 ]\n",
      " [ 0.15784152 -0.01150056  0.04373631]\n",
      " [ 0.0877777   0.41273347 -0.49973345]]\n",
      "step : 128, loss : 0.7607008218765259, W : [[-0.06705526 -0.7927121   2.6640573 ]\n",
      " [ 0.15822801 -0.0115493   0.04339857]\n",
      " [ 0.08896679  0.4125867  -0.50077575]]\n",
      "step : 129, loss : 0.7596569061279297, W : [[-0.07487269 -0.79097706  2.6701398 ]\n",
      " [ 0.15861194 -0.01158938  0.04305473]\n",
      " [ 0.09015305  0.41243175 -0.5018071 ]]\n",
      "step : 130, loss : 0.7586197853088379, W : [[-0.08266535 -0.78924644  2.6762018 ]\n",
      " [ 0.15899323 -0.01162095  0.04270501]\n",
      " [ 0.09133637  0.4122691  -0.50282776]]\n",
      "step : 131, loss : 0.7575893402099609, W : [[-0.09043341 -0.7875201   2.6822436 ]\n",
      " [ 0.15937203 -0.01164441  0.04234967]\n",
      " [ 0.09251685  0.41209888 -0.50383806]]\n",
      "step : 132, loss : 0.7565656900405884, W : [[-0.0981771  -0.7857981   2.6882653 ]\n",
      " [ 0.15974832 -0.01165996  0.04198895]\n",
      " [ 0.09369446  0.41192144 -0.5048382 ]]\n",
      "step : 133, loss : 0.7555484771728516, W : [[-0.10589661 -0.7840802   2.694267  ]\n",
      " [ 0.16012216 -0.01166791  0.04162305]\n",
      " [ 0.09486923  0.411737   -0.5058285 ]]\n",
      "step : 134, loss : 0.7545377612113953, W : [[-0.11359214 -0.7823665   2.700249  ]\n",
      " [ 0.1604936  -0.01166851  0.04125221]\n",
      " [ 0.09604117  0.41154575 -0.5068092 ]]\n",
      "step : 135, loss : 0.7535334825515747, W : [[-0.12126391 -0.7806569   2.706211  ]\n",
      " [ 0.16086262 -0.01166195  0.04087662]\n",
      " [ 0.09721026  0.41134804 -0.50778055]]\n",
      "step : 136, loss : 0.7525354623794556, W : [[-0.12891209 -0.7789513   2.7121537 ]\n",
      " [ 0.16122934 -0.01164857  0.04049653]\n",
      " [ 0.09837656  0.411144   -0.5087428 ]]\n",
      "step : 137, loss : 0.7515438199043274, W : [[-0.13653691 -0.7772497   2.718077  ]\n",
      " [ 0.1615937  -0.01162853  0.04011213]\n",
      " [ 0.09954003  0.41093388 -0.5096961 ]]\n",
      "step : 138, loss : 0.7505582571029663, W : [[-0.14413853 -0.77555203  2.723981  ]\n",
      " [ 0.1619558  -0.01160207  0.03972358]\n",
      " [ 0.10070073  0.4107179  -0.51064086]]\n",
      "step : 139, loss : 0.7495788335800171, W : [[-0.15171716 -0.77385825  2.7298658 ]\n",
      " [ 0.16231564 -0.01156944  0.0393311 ]\n",
      " [ 0.10185865  0.4104963  -0.5115772 ]]\n",
      "step : 140, loss : 0.748605489730835, W : [[-0.15927295 -0.7721683   2.7357316 ]\n",
      " [ 0.16267326 -0.01153082  0.03893487]\n",
      " [ 0.10301383  0.41026923 -0.5125053 ]]\n",
      "step : 141, loss : 0.7476381063461304, W : [[-0.16680613 -0.7704821   2.7415786 ]\n",
      " [ 0.16302866 -0.01148643  0.03853508]\n",
      " [ 0.10416624  0.41003692 -0.5134254 ]]\n",
      "step : 142, loss : 0.7466766238212585, W : [[-0.17431685 -0.7687997   2.747407  ]\n",
      " [ 0.16338192 -0.01143648  0.03813188]\n",
      " [ 0.10531596  0.40979952 -0.5143377 ]]\n",
      "step : 143, loss : 0.7457209825515747, W : [[-0.1818053  -0.767121    2.7532167 ]\n",
      " [ 0.16373299 -0.0113811   0.03772544]\n",
      " [ 0.10646294  0.40955728 -0.51524246]]\n",
      "step : 144, loss : 0.7447711229324341, W : [[-0.18927164 -0.76544595  2.759008  ]\n",
      " [ 0.16408196 -0.01132058  0.03731593]\n",
      " [ 0.10760727  0.4093103  -0.5161398 ]]\n",
      "step : 145, loss : 0.7438268661499023, W : [[-0.19671607 -0.76377445  2.764781  ]\n",
      " [ 0.1644288  -0.01125501  0.03690352]\n",
      " [ 0.10874891  0.40905884 -0.51703   ]]\n",
      "step : 146, loss : 0.742888331413269, W : [[-0.20413876 -0.76210654  2.7705357 ]\n",
      " [ 0.16477358 -0.01118463  0.03648835]\n",
      " [ 0.10988791  0.408803   -0.51791316]]\n",
      "step : 147, loss : 0.7419553995132446, W : [[-0.21153986 -0.76044214  2.7762725 ]\n",
      " [ 0.1651163  -0.01110957  0.03607059]\n",
      " [ 0.11102428  0.40854296 -0.51878947]]\n",
      "step : 148, loss : 0.7410279512405396, W : [[-0.21891956 -0.75878125  2.7819912 ]\n",
      " [ 0.16545697 -0.01103001  0.03565037]\n",
      " [ 0.11215804  0.40827888 -0.51965916]]\n",
      "step : 149, loss : 0.7401059865951538, W : [[-0.226278   -0.7571238   2.7876923 ]\n",
      " [ 0.1657956  -0.01094611  0.03522783]\n",
      " [ 0.1132892   0.40801093 -0.52052236]]\n",
      "step : 150, loss : 0.7391893863677979, W : [[-0.23361537 -0.7554698   2.7933757 ]\n",
      " [ 0.16613226 -0.01085804  0.03480311]\n",
      " [ 0.11441781  0.40773922 -0.52137923]]\n",
      "step : 151, loss : 0.7382780909538269, W : [[-0.24093181 -0.75381917  2.7990415 ]\n",
      " [ 0.1664669  -0.0107659   0.03437633]\n",
      " [ 0.11554385  0.40746394 -0.52222997]]\n",
      "step : 152, loss : 0.7373721599578857, W : [[-0.24822748 -0.7521719   2.80469   ]\n",
      " [ 0.1667996  -0.01066992  0.03394765]\n",
      " [ 0.11666739  0.40718517 -0.52307475]]\n",
      "step : 153, loss : 0.7364714741706848, W : [[-0.25550255 -0.75052786  2.8103209 ]\n",
      " [ 0.1671303  -0.01057015  0.03351718]\n",
      " [ 0.11778837  0.4069031  -0.5239137 ]]\n",
      "step : 154, loss : 0.7355759143829346, W : [[-0.26275718 -0.7488871   2.8159347 ]\n",
      " [ 0.1674591  -0.0104668   0.03308504]\n",
      " [ 0.1189069   0.40661776 -0.5247469 ]]\n",
      "step : 155, loss : 0.7346855401992798, W : [[-0.26999152 -0.7472496   2.8215315 ]\n",
      " [ 0.16778593 -0.01035994  0.03265135]\n",
      " [ 0.12002293  0.4063294  -0.52557456]]\n",
      "step : 156, loss : 0.7338001728057861, W : [[-0.2772057  -0.7456153   2.8271115 ]\n",
      " [ 0.16811088 -0.01024975  0.0322162 ]\n",
      " [ 0.12113655  0.40603808 -0.5263969 ]]\n",
      "step : 157, loss : 0.7329198718070984, W : [[-0.2843999  -0.74398416  2.8326745 ]\n",
      " [ 0.16843392 -0.01013631  0.03177972]\n",
      " [ 0.12224772  0.40574393 -0.5272139 ]]\n",
      "step : 158, loss : 0.732044517993927, W : [[-0.29157424 -0.7423561   2.8382208 ]\n",
      " [ 0.1687551  -0.01001978  0.03134202]\n",
      " [ 0.12335651  0.40544707 -0.52802575]]\n",
      "step : 159, loss : 0.731174111366272, W : [[-0.29872888 -0.74073124  2.8437505 ]\n",
      " [ 0.1690744  -0.00990026  0.0309032 ]\n",
      " [ 0.12446292  0.40514758 -0.5288327 ]]\n",
      "step : 160, loss : 0.7303084135055542, W : [[-0.30586398 -0.7391094   2.8492637 ]\n",
      " [ 0.1693918  -0.00977784  0.03046338]\n",
      " [ 0.12556691  0.4048456  -0.5296347 ]]\n",
      "step : 161, loss : 0.729447603225708, W : [[-0.31297967 -0.7374906   2.8547606 ]\n",
      " [ 0.16970739 -0.00965269  0.03002264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.12666862  0.4045412  -0.530432  ]]\n",
      "step : 162, loss : 0.7285915613174438, W : [[-0.32007608 -0.73587483  2.8602414 ]\n",
      " [ 0.17002113 -0.00952484  0.02958105]\n",
      " [ 0.12776798  0.40423453 -0.53122467]]\n",
      "step : 163, loss : 0.7277402877807617, W : [[-0.32715335 -0.73426205  2.865706  ]\n",
      " [ 0.17033309 -0.00939446  0.02913873]\n",
      " [ 0.1288651   0.40392563 -0.5320129 ]]\n",
      "step : 164, loss : 0.7268936038017273, W : [[-0.33421168 -0.73265225  2.8711545 ]\n",
      " [ 0.17064315 -0.00926158  0.02869579]\n",
      " [ 0.12995985  0.40361464 -0.5327967 ]]\n",
      "step : 165, loss : 0.7260515689849854, W : [[-0.3412511  -0.73104537  2.8765872 ]\n",
      " [ 0.17095147 -0.00912638  0.02825228]\n",
      " [ 0.13105242  0.4033016  -0.5335762 ]]\n",
      "step : 166, loss : 0.7252141237258911, W : [[-0.34827185 -0.7294414   2.882004  ]\n",
      " [ 0.17125796 -0.00898887  0.02780829]\n",
      " [ 0.13214271  0.40298665 -0.5343515 ]]\n",
      "step : 167, loss : 0.7243812084197998, W : [[-0.355274   -0.7278403   2.8874052 ]\n",
      " [ 0.17156269 -0.0088492   0.0273639 ]\n",
      " [ 0.1332308   0.40266985 -0.5351228 ]]\n",
      "step : 168, loss : 0.7235528230667114, W : [[-0.3622577  -0.72624207  2.8927906 ]\n",
      " [ 0.17186563 -0.00870742  0.0269192 ]\n",
      " [ 0.1343167   0.4023513  -0.53589016]]\n",
      "step : 169, loss : 0.7227288484573364, W : [[-0.36922306 -0.7246467   2.8981605 ]\n",
      " [ 0.1721668  -0.00856365  0.02647427]\n",
      " [ 0.13540041  0.40203103 -0.53665364]]\n",
      "step : 170, loss : 0.7219092845916748, W : [[-0.37617022 -0.7230541   2.903515  ]\n",
      " [ 0.1724662  -0.00841797  0.02602918]\n",
      " [ 0.13648199  0.40170917 -0.53741336]]\n",
      "step : 171, loss : 0.721094012260437, W : [[-0.38309932 -0.72146434  2.9088545 ]\n",
      " [ 0.17276385 -0.00827044  0.02558398]\n",
      " [ 0.13756141  0.40138578 -0.5381694 ]]\n",
      "step : 172, loss : 0.7202831506729126, W : [[-0.39001048 -0.7198773   2.9141786 ]\n",
      " [ 0.17305978 -0.00812112  0.02513874]\n",
      " [ 0.13863873  0.40106094 -0.5389219 ]]\n",
      "step : 173, loss : 0.7194764614105225, W : [[-0.39690384 -0.718293    2.9194877 ]\n",
      " [ 0.17335396 -0.00797012  0.02469355]\n",
      " [ 0.13971396  0.40073472 -0.5396709 ]]\n",
      "step : 174, loss : 0.7186740636825562, W : [[-0.4037795  -0.71671146  2.9247818 ]\n",
      " [ 0.17364642 -0.00781751  0.02424846]\n",
      " [ 0.14078712  0.40040714 -0.5404165 ]]\n",
      "step : 175, loss : 0.7178759574890137, W : [[-0.4106376  -0.7151326   2.930061  ]\n",
      " [ 0.17393716 -0.00766331  0.02380355]\n",
      " [ 0.14185822  0.40007836 -0.5411588 ]]\n",
      "step : 176, loss : 0.7170817852020264, W : [[-0.41747823 -0.7135564   2.9353256 ]\n",
      " [ 0.17422622 -0.00750767  0.02335886]\n",
      " [ 0.14292732  0.39974833 -0.5418979 ]]\n",
      "step : 177, loss : 0.7162919044494629, W : [[-0.42430156 -0.71198285  2.9405754 ]\n",
      " [ 0.1745135  -0.00735056  0.02291446]\n",
      " [ 0.14399433  0.39941722 -0.54263383]]\n",
      "step : 178, loss : 0.7155060172080994, W : [[-0.43110767 -0.71041197  2.9458106 ]\n",
      " [ 0.17479919 -0.00719218  0.0224704 ]\n",
      " [ 0.14505945  0.39908496 -0.5433667 ]]\n",
      "step : 179, loss : 0.7147241830825806, W : [[-0.4378967  -0.70884365  2.9510312 ]\n",
      " [ 0.17508309 -0.00703239  0.02202673]\n",
      " [ 0.14612249  0.3987518  -0.5440965 ]]\n",
      "step : 180, loss : 0.7139463424682617, W : [[-0.4446687  -0.70727795  2.9562376 ]\n",
      " [ 0.1753654  -0.0068715   0.02158351]\n",
      " [ 0.14718367  0.39841756 -0.54482347]]\n",
      "step : 181, loss : 0.7131724953651428, W : [[-0.45142388 -0.7057148   2.9614296 ]\n",
      " [ 0.17564593 -0.00670931  0.02114079]\n",
      " [ 0.1482428   0.39808255 -0.5455476 ]]\n",
      "step : 182, loss : 0.7124024629592896, W : [[-0.45816228 -0.70415425  2.9666073 ]\n",
      " [ 0.17592493 -0.00654612  0.02069862]\n",
      " [ 0.14930014  0.39774656 -0.54626894]]\n",
      "step : 183, loss : 0.7116364240646362, W : [[-0.46488407 -0.7025962   2.971771  ]\n",
      " [ 0.17620212 -0.00638175  0.02025705]\n",
      " [ 0.15035546  0.3974099  -0.5469876 ]]\n",
      "step : 184, loss : 0.7108741998672485, W : [[-0.47158933 -0.7010407   2.9769208 ]\n",
      " [ 0.17647776 -0.00621648  0.01981612]\n",
      " [ 0.15140899  0.39707235 -0.54770356]]\n",
      "step : 185, loss : 0.7101158499717712, W : [[-0.47827816 -0.6994877   2.9820566 ]\n",
      " [ 0.1767517  -0.00605015  0.01937586]\n",
      " [ 0.15246059  0.3967342  -0.54841703]]\n",
      "step : 186, loss : 0.7093612551689148, W : [[-0.4849507  -0.69793713  2.9871786 ]\n",
      " [ 0.17702404 -0.00588298  0.01893635]\n",
      " [ 0.15351039  0.39639533 -0.54912794]]\n",
      "step : 187, loss : 0.7086104154586792, W : [[-0.49160704 -0.6963891   2.992287  ]\n",
      " [ 0.17729472 -0.0057149   0.0184976 ]\n",
      " [ 0.15455833  0.39605588 -0.54983646]]\n",
      "step : 188, loss : 0.7078632116317749, W : [[-0.49824727 -0.6948435   2.9973814 ]\n",
      " [ 0.17756379 -0.00554601  0.01805966]\n",
      " [ 0.15560448  0.39571586 -0.5505426 ]]\n",
      "step : 189, loss : 0.7071197032928467, W : [[-0.50487155 -0.6933003   3.0024626 ]\n",
      " [ 0.17783122 -0.00537638  0.01762258]\n",
      " [ 0.15664881  0.39537528 -0.55124635]]\n",
      "step : 190, loss : 0.7063800096511841, W : [[-0.5114799  -0.6917595   3.0075302 ]\n",
      " [ 0.17809704 -0.00520598  0.01718637]\n",
      " [ 0.15769137  0.39503425 -0.5519479 ]]\n",
      "step : 191, loss : 0.7056437134742737, W : [[-0.5180725  -0.6902212   3.0125844 ]\n",
      " [ 0.17836127 -0.00503492  0.01675109]\n",
      " [ 0.15873219  0.39469275 -0.5526472 ]]\n",
      "step : 192, loss : 0.7049111127853394, W : [[-0.5246494  -0.68868524  3.0176253 ]\n",
      " [ 0.17862387 -0.00486319  0.01631676]\n",
      " [ 0.15977123  0.39435086 -0.5533443 ]]\n",
      "step : 193, loss : 0.7041820287704468, W : [[-0.5312107  -0.6871516   3.022653  ]\n",
      " [ 0.17888491 -0.0046909   0.01588342]\n",
      " [ 0.1608086   0.39400855 -0.55403936]]\n",
      "step : 194, loss : 0.7034565210342407, W : [[-0.53775656 -0.68562037  3.0276678 ]\n",
      " [ 0.1791443  -0.00451797  0.0154511 ]\n",
      " [ 0.16184416  0.39366594 -0.5547323 ]]\n",
      "step : 195, loss : 0.7027344107627869, W : [[-0.544287   -0.68409145  3.0326693 ]\n",
      " [ 0.1794022  -0.00434456  0.01501982]\n",
      " [ 0.16287814  0.39332297 -0.5554233 ]]\n",
      "step : 196, loss : 0.7020158171653748, W : [[-0.55080223 -0.68256485  3.037658  ]\n",
      " [ 0.17965843 -0.0041706   0.01458964]\n",
      " [ 0.16391034  0.39297977 -0.55611235]]\n",
      "step : 197, loss : 0.7013006210327148, W : [[-0.55730224 -0.6810406   3.0426338 ]\n",
      " [ 0.17991315 -0.00399622  0.01416056]\n",
      " [ 0.16494094  0.3926363  -0.5567995 ]]\n",
      "step : 198, loss : 0.7005888223648071, W : [[-0.56378716 -0.67951864  3.0475967 ]\n",
      " [ 0.18016626 -0.00382138  0.01373261]\n",
      " [ 0.16596985  0.39229262 -0.5574847 ]]\n",
      "step : 199, loss : 0.6998804807662964, W : [[-0.5702571  -0.67799896  3.052547  ]\n",
      " [ 0.18041784 -0.00364616  0.01330582]\n",
      " [ 0.16699713  0.39194876 -0.5581681 ]]\n",
      "step : 200, loss : 0.6991753578186035, W : [[-0.5767122  -0.67648154  3.0574846 ]\n",
      " [ 0.18066785 -0.00347056  0.01288022]\n",
      " [ 0.1680228   0.39160472 -0.55884975]]\n",
      "prediction : [2 2 2]\n",
      "accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(201):\n",
    "        loss_val, W_val,_ = sess.run([loss, W, training_op], feed_dict = {X :X_data, Y: Y_data})\n",
    "        print(\"step : {}, loss : {}, W : {}\".format(step, loss_val, W_val))\n",
    "    prediction_val = sess.run(prediction , feed_dict = {X: X_test, Y : Y_test})\n",
    "    print(\"prediction : {}\".format(prediction_val))\n",
    "    accuracy_val = sess.run(accuracy , feed_dict = {X: X_test, Y: Y_test})\n",
    "    print(\"accuracy : {}\".format(accuracy_val))\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "scaler = StandardScaler()\n",
    "XY = scaler.fit_transform(XY)\n",
    "X_data = XY[:, 0:-1]\n",
    "y_data = XY[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(X,W) + b\n",
    "loss = tf.reduce_mean(tf.square(logits - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 Loss:  10.136242 \n",
      "Prediction:\n",
      " Tensor(\"add_4:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "init= tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(200):\n",
    "        loss_val, logits_val, _ = sess.run(\n",
    "        [loss, logits, training_op], feed_dict={X: X_data, y: y_data})\n",
    "    print(step, \"Loss: \", loss_val, \"\\nPrediction:\\n\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 Loss:  2.157516 \n",
      "Prediction:\n",
      " Tensor(\"add_5:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0) -np.min(data,0)\n",
    "    return numerator / denominator\n",
    "XY = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "XY = min_max_scaler(XY)\n",
    "X_data = XY[:, 0:-1]\n",
    "y_data = XY[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "logits = tf.matmul(X,W) + b\n",
    "loss = tf.reduce_mean(tf.square(logits - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init= tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(200):\n",
    "        loss_val, logits_val, _ = sess.run(\n",
    "        [loss, logits, training_op], feed_dict={X: X_data, y: y_data})\n",
    "    print(step, \"Loss: \", loss_val, \"\\nPrediction:\\n\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "X_batch, Y_batch = MNIST.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 784), (100, 10))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape, Y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 0.8906\n",
      "테스트 이미지의 실제 레이블 : 5\n",
      "예측 레이블 : 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "n_inputs = 28*28\n",
    "n_classes = 10\n",
    "X = tf.placeholder(tf.float32, shape= [None, n_inputs])\n",
    "Y = tf.placeholder(tf.float32, shape = [None , n_classes])\n",
    "W = tf.Variable(tf.random_normal([n_inputs, n_classes]))\n",
    "b = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "logits = tf.nn.softmax(tf.matmul(X,W) + b)\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(logits), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "\n",
    "n_epochs = 15\n",
    "n_iterations = int(MNIST.train.num_examples / 100)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_iterations):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(100)\n",
    "            loss_val , _  = sess.run([loss, training_op], feed_dict = {X:X_batch, Y: Y_batch})\n",
    "       \n",
    "    accuracy_val = sess.run(accuracy, feed_dict = {X: MNIST.test.images, Y: MNIST.test.labels})\n",
    "    print(\"정확도\", accuracy_val)\n",
    "    \n",
    "    r = random.randint(0,MNIST.test.num_examples -1)\n",
    "    test_image  = MNIST.test.images[r:r+1]\n",
    "    test_image_label = np.argmax(MNIST.test.labels[r])\n",
    "    print(\"테스트 이미지의 실제 레이블 : {}\".format(test_image_label))\n",
    "    plt.imshow(test_image.reshape(28,28), cmap = 'binary', interpolation= 'nearest')\n",
    "    test_image_pred = sess.run(logits, feed_dict= {X: test_image})\n",
    "    print('예측 레이블 : {}'.format(np.argmax(test_image_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax_12:0' shape=() dtype=int64>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(MNIST.test.labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = MNIST.test.images[3].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e6bf2fa400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST.test.labels[r]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
