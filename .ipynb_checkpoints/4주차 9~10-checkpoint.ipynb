{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1 : XOR을 위한 텐서플로우 딥네트웍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None,2], name = \"X\")\n",
    "y=  tf.placeholder(tf.float32, shape = [None,1], name = 'y')\n",
    "W1 = tf.Variable(tf.random_normal([2,2]) ,name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "W2 = tf.Variable(tf.random_normal([2,1]) ,name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "\n",
    "layer1_outputs  = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "logits = tf.sigmoid(tf.matmul(layer1_outputs,W2)+b2)\n",
    "\n",
    "loss = -tf.reduce_mean(y*tf.log(logits) + (1-y) * tf.log(1-logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate  = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.cast(logits > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(correct, y), dtype = tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(10000):\n",
    "        sess.run(training_op, feed_dict  ={X:X_data, y:y_data})\n",
    "    acc_val = sess.run(accuracy, feed_dict = {X:X_data, y: y_data})\n",
    "    print(\"정확도 : {}\".format(acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2 텐서보드  \n",
    "1. 텐서플로우 그래프로부터 기록할 텐서를 정한다.\n",
    "2. 모든 요약그래프를 합침\n",
    "3. 쓰고 추가하는 그래프를 생성\n",
    "4. 요약 그래프와 추가 그래프를 실행\n",
    "5. 텐서보드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.5\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None,2], name = \"X\")\n",
    "y=  tf.placeholder(tf.float32, shape = [None,1], name = 'y')\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([2,2]) ,name = \"W1\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name = 'b1')\n",
    "    layer1_outputs = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", layer1_outputs)\n",
    "    \n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2,1]) ,name = \"W2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name = 'b2')\n",
    "    logits = tf.sigmoid(tf.matmul(layer1_outputs,W2)+b2)\n",
    "    \n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"logits\", logits)\n",
    "    \n",
    "with tf.name_scope(\"Loss\"):\n",
    "    loss = -tf.reduce_mean(y*tf.log(logits) + (1-y) * tf.log(1-logits))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate  = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "correct = tf.cast(logits > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(correct, y), dtype = tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./tensorboard/4주차_실습2\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    for step in range(1000):\n",
    "        summary , _ = sess.run([merged_summary, training_op], feed_dict  ={X:X_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step = step)\n",
    "        \n",
    "    acc_val = sess.run(accuracy, feed_dict = {X:X_data, y: y_data})\n",
    "    print(\"정확도 : {}\".format(acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 3 MNIST로 98%이상 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "MNIST = fetch_openml(\"mnist_784\", version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = MNIST['data']\n",
    "y = MNIST['target']\n",
    "X_train,X_test , y_train, y_test = train_test_split(X,y, test_size = 10000, random_state = 42)\n",
    "X_train,X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 6000,\n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(42)\n",
    "def fetch_batch(X,y ,batch_size):\n",
    "    rnd_indices = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for rnd_idx in np.array_split(rnd_indices, n_batches):\n",
    "        X_batch, y_batch = X[rnd_idx], y[rnd_idx]\n",
    "        yield X_batch, y_batch\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D339A9C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D339A9C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D339A9C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D339A9C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D4401AF98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D4401AF98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D4401AF98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D4401AF98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-9-9df9398a323a>:32: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D33B17940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D33B17940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D33B17940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D33B17940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D43CE0048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D43CE0048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D43CE0048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D43CE0048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D44C84B00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D44C84B00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D44C84B00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000028D44C84B00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000028D44C59BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D35DA3C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D35DA3C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D35DA3C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028D35DA3C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "epoch : 0, training_loss : 0.5235682129859924\n",
      "          , valid_loss : 0.19940674304962158\n",
      "          , acc_loss : 0.9418333172798157\n",
      "epoch : 1, training_loss : 0.3586609959602356\n",
      "          , valid_loss : 0.1658446192741394\n",
      "          , acc_loss : 0.9490000009536743\n",
      "epoch : 2, training_loss : 0.3626793324947357\n",
      "          , valid_loss : 0.14023642241954803\n",
      "          , acc_loss : 0.9559999704360962\n",
      "epoch : 3, training_loss : 0.25709059834480286\n",
      "          , valid_loss : 0.1370689570903778\n",
      "          , acc_loss : 0.9568333625793457\n",
      "epoch : 4, training_loss : 0.2952166497707367\n",
      "          , valid_loss : 0.12868575751781464\n",
      "          , acc_loss : 0.9620000123977661\n",
      "epoch : 5, training_loss : 0.280065655708313\n",
      "          , valid_loss : 0.12324398756027222\n",
      "          , acc_loss : 0.9629999995231628\n",
      "epoch : 6, training_loss : 0.2966149151325226\n",
      "          , valid_loss : 0.12225668132305145\n",
      "          , acc_loss : 0.9624999761581421\n",
      "epoch : 7, training_loss : 0.28330284357070923\n",
      "          , valid_loss : 0.11628010123968124\n",
      "          , acc_loss : 0.9648333191871643\n",
      "epoch : 8, training_loss : 0.3731203079223633\n",
      "          , valid_loss : 0.1101064383983612\n",
      "          , acc_loss : 0.9653333425521851\n",
      "epoch : 9, training_loss : 0.5010164976119995\n",
      "          , valid_loss : 0.11799970269203186\n",
      "          , acc_loss : 0.9608333110809326\n",
      "epoch : 10, training_loss : 0.404979944229126\n",
      "          , valid_loss : 0.10711051523685455\n",
      "          , acc_loss : 0.9679999947547913\n",
      "epoch : 11, training_loss : 0.24896320700645447\n",
      "          , valid_loss : 0.1094333678483963\n",
      "          , acc_loss : 0.9663333296775818\n",
      "epoch : 12, training_loss : 0.20410273969173431\n",
      "          , valid_loss : 0.09848484396934509\n",
      "          , acc_loss : 0.9698333144187927\n",
      "epoch : 13, training_loss : 0.1924724578857422\n",
      "          , valid_loss : 0.10733837634325027\n",
      "          , acc_loss : 0.9661666750907898\n",
      "epoch : 14, training_loss : 0.21701735258102417\n",
      "          , valid_loss : 0.10078458487987518\n",
      "          , acc_loss : 0.968666672706604\n",
      "epoch : 15, training_loss : 0.3129447102546692\n",
      "          , valid_loss : 0.0919608548283577\n",
      "          , acc_loss : 0.9733333587646484\n",
      "epoch : 16, training_loss : 0.22875791788101196\n",
      "          , valid_loss : 0.09569323807954788\n",
      "          , acc_loss : 0.9706666469573975\n",
      "epoch : 17, training_loss : 0.17793132364749908\n",
      "          , valid_loss : 0.09397605061531067\n",
      "          , acc_loss : 0.9714999794960022\n",
      "epoch : 18, training_loss : 0.2298266440629959\n",
      "          , valid_loss : 0.0897953063249588\n",
      "          , acc_loss : 0.9708333611488342\n",
      "epoch : 19, training_loss : 0.27352243661880493\n",
      "          , valid_loss : 0.10049334913492203\n",
      "          , acc_loss : 0.9693333506584167\n",
      "epoch : 20, training_loss : 0.2495124191045761\n",
      "          , valid_loss : 0.09192077815532684\n",
      "          , acc_loss : 0.9714999794960022\n",
      "epoch : 21, training_loss : 0.18993979692459106\n",
      "          , valid_loss : 0.08939650654792786\n",
      "          , acc_loss : 0.971833348274231\n",
      "epoch : 22, training_loss : 0.2594756782054901\n",
      "          , valid_loss : 0.0868552029132843\n",
      "          , acc_loss : 0.9729999899864197\n",
      "epoch : 23, training_loss : 0.24928335845470428\n",
      "          , valid_loss : 0.08753281831741333\n",
      "          , acc_loss : 0.972000002861023\n",
      "epoch : 24, training_loss : 0.2313624620437622\n",
      "          , valid_loss : 0.09056463837623596\n",
      "          , acc_loss : 0.9728333353996277\n",
      "epoch : 25, training_loss : 0.34132087230682373\n",
      "          , valid_loss : 0.09591548889875412\n",
      "          , acc_loss : 0.9696666598320007\n",
      "epoch : 26, training_loss : 0.18273331224918365\n",
      "          , valid_loss : 0.0859423354268074\n",
      "          , acc_loss : 0.9733333587646484\n",
      "epoch : 27, training_loss : 0.18190258741378784\n",
      "          , valid_loss : 0.08247518539428711\n",
      "          , acc_loss : 0.9754999876022339\n",
      "epoch : 28, training_loss : 0.29059135913848877\n",
      "          , valid_loss : 0.07830008864402771\n",
      "          , acc_loss : 0.9760000109672546\n",
      "epoch : 29, training_loss : 0.15998752415180206\n",
      "          , valid_loss : 0.08200941234827042\n",
      "          , acc_loss : 0.9743333458900452\n",
      "epoch : 30, training_loss : 0.23180562257766724\n",
      "          , valid_loss : 0.08464545756578445\n",
      "          , acc_loss : 0.9738333225250244\n",
      "epoch : 31, training_loss : 0.3332258462905884\n",
      "          , valid_loss : 0.08900641649961472\n",
      "          , acc_loss : 0.971833348274231\n",
      "epoch : 32, training_loss : 0.3124615550041199\n",
      "          , valid_loss : 0.07870402932167053\n",
      "          , acc_loss : 0.9743333458900452\n",
      "epoch : 33, training_loss : 0.2730250656604767\n",
      "          , valid_loss : 0.08135568350553513\n",
      "          , acc_loss : 0.9750000238418579\n",
      "조기종료!\n",
      "최종 loss : 0.09230777621269226, 최종 accuracy: 0.9714999794960022\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28*28\n",
    "n_outputs = 10\n",
    "n_layers = 3\n",
    "batch_norm_momentum = 0.9\n",
    "dropout_rate = 0.6\n",
    "n_neurons = 512\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "\n",
    "best_loss = np.inf\n",
    "count = 0\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs], name = \"X\")\n",
    "y = tf.placeholder(tf.int32, shape = [None], name = \"y\")\n",
    "\n",
    "if(batch_norm_momentum or dropout_rate):\n",
    "    training = tf.placeholder_with_default(False, shape = [], name = 'training')\n",
    "else:\n",
    "    training = None\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "inputs = X\n",
    "for layer in range(n_layers):\n",
    "    if dropout_rate:\n",
    "        inputs = tf.layers.dropout(inputs, dropout_rate, training = training)\n",
    "    inputs = tf.layers.dense(inputs, n_neurons, kernel_initializer = he_init, \n",
    "                        name = \"hidden%d\" %(layer+1))\n",
    "    if batch_norm_momentum:\n",
    "        inputs = tf.layers.batch_normalization(inputs,momentum = batch_norm_momentum, \n",
    "                                               training = training)\n",
    "    inputs = tf.nn.relu(inputs, name = 'hidden%d_out' %(layer +1))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# if(dropout_rate):\n",
    "#     d1 = tf.layers.dropout(X, dropout_rate, training = training)\n",
    "# l1 = tf.layers.dense(d1, n_neurons, kernel_initializer = he_init, \n",
    "#                         name = \"hidden1\" )\n",
    "# if(batch_norm_momentum):\n",
    "#     b1 = tf.layers.batch_normalization(l1,momentum = batch_norm_momentum, training = training)\n",
    "# r1 = tf.nn.relu(b1, name = 'hidden1_out' )\n",
    "# if(dropout_rate):\n",
    "#     d2 = tf.layers.dropout(r1, dropout_rate, training = training)\n",
    "# l2 = tf.layers.dense(d2, n_neurons, kernel_initializer = he_init, \n",
    "#                         name = \"hidden2\")\n",
    "# if(batch_norm_momentum):\n",
    "#     b2 = tf.layers.batch_normalization(l2,momentum = batch_norm_momentum, training = training)\n",
    "# r2 = tf.nn.relu(b2, name = 'hidden2_out')\n",
    "# if(dropout_rate):\n",
    "#     d3 = tf.layers.dropout(r2, dropout_rate, training = training)\n",
    "# l3 = tf.layers.dense(d3, n_neurons, kernel_initializer = he_init, \n",
    "#                         name = \"hidden3\")\n",
    "# if(batch_norm_momentum):\n",
    "#     b3 = tf.layers.batch_normalization(l3,momentum = batch_norm_momentum, training = training)\n",
    "# r3 = tf.nn.relu(b3, name = 'hidden3_out')\n",
    "\n",
    "\n",
    "\n",
    "logits  = tf.layers.dense(inputs,n_outputs, kernel_initializer = he_init, name = \"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name = 'Y_proba')\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in fetch_batch(X_train, y_train, batch_size):\n",
    "            feed_dict = {X:X_batch , y:y_batch}\n",
    "            if(training is not None):\n",
    "                feed_dict[training] = True\n",
    "            train_loss_val, _ = sess.run([loss,training_op], \n",
    "                                         feed_dict = feed_dict)\n",
    "            if extra_update_ops:\n",
    "                sess.run(extra_update_ops, feed_dict = feed_dict)\n",
    "        valid_loss_val,valid_acc_val = sess.run([loss,accuracy], feed_dict = {X:X_valid, y:y_valid})\n",
    "        if(valid_loss_val < best_loss):\n",
    "            best_loss = valid_loss_val\n",
    "            \n",
    "            count= 0\n",
    "        else:\n",
    "            if(count >= 5):\n",
    "                print(\"조기종료!\")\n",
    "                break\n",
    "            count +=1\n",
    "        \n",
    "        print(\"epoch : {}, training_loss : {}\".format(epoch , train_loss_val))\n",
    "        print(\"          , valid_loss : {}\".format(valid_loss_val))\n",
    "        print(\"          , valid_acc : {}\".format(valid_acc_val))\n",
    "    final_loss, final_acc = sess.run([loss ,accuracy], feed_dict = {X:X_test, y:y_test})\n",
    "    print(\"최종 loss : {}, 최종 accuracy: {}\".format(final_loss, final_acc))\n",
    "          \n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
